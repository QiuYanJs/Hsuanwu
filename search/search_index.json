{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hsuanwu - Overview","text":"<p>Hsuanwu: Long-Term Evolution Project of Reinforcement Learning is inspired by the long-term evolution (LTE) standard project in telecommunications, which aims to track the latest research progress in reinforcement learning (RL) and provide stable and efficient baselines. In Hsuanwu, you can find everything you need in RL, such as training, evaluation, deployment, etc. The highlight features of Hsuanwu:</p> <ul> <li>\u23f1\ufe0f Latest algorithms and tricks;</li> <li>\ud83e\uddf1 Highly modularized design for complete decoupling of RL algorithms;</li> <li>\ud83d\ude80 Optimized workflow for full hardware acceleration;</li> <li>\u2699\ufe0f Support for custom environments;</li> <li>\ud83d\udda5\ufe0f Support for multiple computing devices like GPU and NPU;</li> <li>\ud83d\udee0\ufe0f Support for RL model engineering deployment (TensorRT, CANN, ...);</li> <li>\ud83d\udcbe Large number of reusable bechmarks (See HsuanwuHub);</li> <li>\ud83d\udccb Elegant experimental management powered by Hydra.</li> </ul> <p>See the project structure below:</p> <p>Hsuanwu evolves based on reinforcement learning algorithms and integrates latest tricks. The following figure demonstrates the main evolution roadmap of Hsuanwu:</p> <p>Join the developer community for issues and discussions:</p> Slack QQ GitHub"},{"location":"api/","title":"Overview","text":""},{"location":"api/#common-auxiliary-modules-like-trainer-and-logger","title":"Common: Auxiliary modules like trainer and logger.","text":"<ul> <li>Engine: Engine for building Hsuanwu application.</li> <li>Logger: Logger for managing output information.</li> </ul>"},{"location":"api/#xploit-modules-that-focus-on-exploitation-in-rl","title":"Xploit: Modules that focus on exploitation in RL.","text":"<ul> <li>Agent: Agent for interacting and learning.</li> </ul> Module Recurrent Box Discrete MultiBinary Multi Processing NPU Paper Citations SAC \u274c \u2714\ufe0f \u274c \u274c \u274c \ud83d\udc0c Link 5077\u2b50 DrQ \u274c \u2714\ufe0f \u274c \u274c \u274c \ud83d\udc0c Link 433\u2b50 DDPG \u274c \u2714\ufe0f \u274c \u274c \u274c \ud83d\udc0c Link 11819\u2b50 DrQ-v2 \u274c \u2714\ufe0f \u274c \u274c \u274c \ud83d\udc0c Link 100\u2b50 DAAC \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \ud83d\udc0c Link 56\u2b50 PPO \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \ud83d\udc0c Link 11155\u2b50 DrAC \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \ud83d\udc0c Link 29\u2b50 PPG \u274c \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f \ud83d\udc0c Link 82\u2b50 IMPALA \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u274c \u2714\ufe0f \ud83d\udc0c Link 1219\u2b50 <p>Tips of Agent</p> <ul> <li>\ud83d\udc0c: Developing.</li> <li>NPU: Support Neural-network processing unit.</li> <li>Recurrent: Support recurrent neural network.</li> <li>Box: A N-dimensional box that containes every point in the action space.</li> <li>Discrete: A list of possible actions, where each timestep only one of the actions can be used.</li> <li>MultiBinary: A list of possible actions, where each timestep any of the actions can be used in any combination.</li> </ul> <ul> <li>Encoder: Neural nework-based encoder for processing observations.</li> </ul> Module Input Reference Target Task EspeholtResidualEncoder Images IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures Atari or Procgen games. IdentityEncoder States N/A DeepMind Control Suite: state MnihCnnEncoder Images Playing Atari with Deep Reinforcement Learning Atari games. TassaCnnEncoder Images DeepMind Control Suite DeepMind Control Suite: pixel VanillaMlpEncoder States N/A DeepMind Control Suite: state <p>Tips of Encoder</p> <ul> <li>Naming Rule: 'Surname of the first author' + 'Backbone' + 'Encoder'</li> <li>Input: Input type.</li> <li>Target Task: The testing tasks in their paper or potential tasks.</li> </ul> <ul> <li>Storage: Storge for storing collected experiences.</li> </ul> Module Remark DecoupledRolloutStorage On-Policy RL VanillaRolloutStorage On-Policy RL VanillaReplayStorage Off-Policy RL NStepReplayStorage Off-Policy RL PrioritizedReplayStorage Off-Policy RL DistributedStorage Distributed RL"},{"location":"api/#xplore-modules-that-focus-on-exploration-in-rl","title":"Xplore: Modules that focus on exploration in RL.","text":"<ul> <li>Augmentation: PyTorch.nn-like modules for observation augmentation.</li> </ul> Module Input Reference GaussianNoise States Reinforcement Learning with Augmented Data RandomAmplitudeScaling States Reinforcement Learning with Augmented Data AutoAugment Images torchvision ElasticTransform Images torchvision GrayScale Images Reinforcement Learning with Augmented Data RandomAdjustSharpness Images torchvision RandomAugment Images torchvision RandomAutocontrast Images torchvision RandomColorJitter Images Reinforcement Learning with Augmented Data RandomConvolution Images Reinforcement Learning with Augmented Data RandomCrop Images Reinforcement Learning with Augmented Data RandomCutout Images Reinforcement Learning with Augmented Data RandomCutoutColor Images Reinforcement Learning with Augmented Data RandomEqualize Images torchvision RandomFlip Images Reinforcement Learning with Augmented Data RandomInvert Images torchvision RandomPerspective Images torchvision RandomRotate Images Reinforcement Learning with Augmented Data RandomShift Images Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning RandomTranslate Images Reinforcement Learning with Augmented Data <ul> <li>Distribution: Distributions for sampling actions.</li> </ul> Module Type Reference NormalNoise Noise torch.distributions OrnsteinUhlenbeckNoise Noise Continuous Control with Deep Reinforcement Learning TruncatedNormalNoise Noise Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning Bernoulli Distribution torch.distributions Categorical Distribution torch.distributions DiagonalGaussian Distribution torch.distributions SquashedNormal Distribution torch.distributions <p>Tips of Distribution</p> <ul> <li> <p>In Hsuanwu, the action noise is implemented via a <code>Distribution</code> manner to realize unification.</p> </li> <li> <p>Reward: Intrinsic reward modules for enhancing exploration.</p> </li> </ul> Module Remark Repr. Visual Reference PseudoCounts Count-Based exploration \u2714\ufe0f \u2714\ufe0f Never Give Up: Learning Directed Exploration Strategies ICM Curiosity-driven exploration \u2714\ufe0f \u2714\ufe0f Curiosity-Driven Exploration by Self-Supervised Prediction RND Count-based exploration \u274c \u2714\ufe0f Exploration by Random Network Distillation GIRM Curiosity-driven exploration \u2714\ufe0f \u2714\ufe0f Intrinsic Reward Driven Imitation Learning via Generative Model NGU Memory-based exploration \u2714\ufe0f \u2714\ufe0f Never Give Up: Learning Directed Exploration Strategies RIDE Procedurally-generated environment \u2714\ufe0f \u2714\ufe0f RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments RE3 Entropy Maximization \u274c \u2714\ufe0f State Entropy Maximization with Random Encoders for Efficient Exploration RISE Entropy Maximization \u274c \u2714\ufe0f R\u00e9nyi State Entropy Maximization for Exploration Acceleration in Reinforcement Learning REVD Divergence Maximization \u274c \u2714\ufe0f Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning <p>Tips of Reward</p> <ul> <li>\ud83d\udc0c: Developing.</li> <li>Repr.: The method involves representation learning.</li> <li>Visual: The method works well in visual RL.</li> </ul> <p>See Tutorials: Use intrinsic reward and observation augmentation for usage examples.</p>"},{"location":"api/#evaluation-reasonable-and-reliable-metrics-for-algorithm-evaluation","title":"Evaluation: Reasonable and reliable metrics for algorithm evaluation.","text":"<p>See Tutorials: Evaluate your model.</p>"},{"location":"api/#env-packaged-environments-eg-atari-games-for-fast-invocation","title":"Env: Packaged environments (e.g., Atari games) for fast invocation.","text":"Module Name Remark Reference make_atari_env Atari Games Discrete control The Arcade Learning Environment: An Evaluation Platform for General Agents make_bullet_env PyBullet Robotics Environments Continuous control Pybullet: A Python Module for Physics Simulation for Games, Robotics and Machine Learning make_dmc_env DeepMind Control Suite Continuous control DeepMind Control Suite make_procgen_env Procgen Games Discrete control Leveraging Procedural Generation to Benchmark Reinforcement Learning make_minigrid_env MiniGrid Games Discrete control Minimalistic Gridworld Environment for Gymnasium"},{"location":"api/#pre-training-methods-of-pre-training-in-rl","title":"Pre-training: Methods of pre-training in RL.","text":"<p>See Tutorials: Pre-training in Hsuanwu.</p>"},{"location":"api/#deployment-methods-of-model-deployment-in-rl","title":"Deployment: Methods of model deployment in RL.","text":"<p>See Tutorials: Deploy your model in inference devices.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"benchmarks/#hsuanwuhub","title":"HsuanwuHub","text":"<p>Hsuanwu provides a large number of reusable data and models of representative RL benchmarks. Install <code>HsuanwuHub</code> by  <pre><code>pip install hsuanwuhub\n</code></pre> Suppose we want to evaluate algorithm performance on the Procgen benchmark. example.py<pre><code>from hsuanwuhub import datasets\nprocgen = datasets.Procgen()\nprocgen_scores = procgen.load_scores()\nprint(procgen_scores['PPO'].shape)\n# Output:\n# (10, 16)\n</code></pre> For each algorithm, this will return a <code>NdArray</code> of size (<code>10</code> x <code>16</code>) where scores[n][m] represent the score on run <code>n</code> of task <code>m</code>.</p>"},{"location":"benchmarks/#support-list","title":"Support List","text":"Benchmark Training Steps Scores Curves Models Atari Games 100K \u2714\ufe0f \u2714\ufe0f \ud83d\udc0c PyBullet Robotics Environments \ud83d\udc0c \ud83d\udc0c \ud83d\udc0c \ud83d\udc0c DeepMind Control Suite 500K \u2714\ufe0f \u2714\ufe0f \ud83d\udc0c Procgen Games 25M \u2714\ufe0f \ud83d\udc0c \ud83d\udc0c MiniGrid Games \ud83d\udc0c \ud83d\udc0c \ud83d\udc0c \ud83d\udc0c <p>Tip</p> <ul> <li>\ud83d\udc0c: Incoming.</li> <li>Scores: Available final scores.</li> <li>Curves: Available training curves.</li> <li>Visual: Available trained models.</li> </ul> <p>Find all the benchmark results in https://hub.hsuanwu.dev/.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001","title":"v0.0.1","text":"<p>05/05/2023 (Version 0.0.1)</p> <p>Version 0.0.1 published.</p> <ul> <li>New features:<ul> <li>Agent: SAC, DrQ, DDPG, DrQ-v2, PPO, DAAC, DrAC, PPG, IMPALA</li> <li>Encoder: EspeholtResidualEncoder, IdentityEncoder, MnihCnnEncoder, TassaCnnEncoder, VanillaMlpEncoder</li> <li>Storage: DecoupledRolloutStorage, VanillaRolloutStorage, VanillaReplayStorage, NStepReplayStorage, PrioritizedReplayStorage, DistributedStorage</li> <li>Augmentation: GaussianNoise, RandomAmplitudeScaling, RandomShift, ...</li> <li>Distribution: TruncatedNormalNoise, Bernoulli, Categorical, DiagonalGaussian, ...</li> <li>Reward: PseudoCounts, ICM, RND, RE3, ...</li> </ul> </li> </ul>"},{"location":"changelog/#initialization","title":"Initialization","text":"<p>19/01/2023</p> <ul> <li>Repository initialization and first commit.</li> </ul>"},{"location":"contributing/","title":"Contributing to Hsuanwu","text":"<p>Thank you for using and contributing to Hsuanwu project!!!\ud83d\udc4b\ud83d\udc4b\ud83d\udc4b Before you begin writing code, it is important that you share your intention to contribute with the team, based on the type of contribution:</p> <ol> <li> <p>You want to propose a new feature and implement it:</p> <ul> <li>Post about your intended feature in an issue, and we shall discuss the design and implementation. Once we agree that the plan looks good, go ahead and implement it.</li> </ul> </li> <li> <p>You want to implement a feature or bug-fix for an outstanding issue:</p> <ul> <li>Search for your issue in the Hsuanwu issue list.</li> <li>Pick an issue and comment that you'd like to work on the feature or bug-fix.</li> <li>If you need more context on a particular issue, please ask and we shall provide.</li> </ul> </li> </ol> <p>Once you implement and test your feature or bug-fix, please submit a Pull Request to https://github.com/RLE-Foundation/Hsuanwu.</p>"},{"location":"contributing/#get-hsuanwu","title":"Get Hsuanwu","text":"<p>Open up a terminal and clone the repository from GitHub with <code>git</code>: <pre><code>git clone https://github.com/RLE-Foundation/Hsuanwu.git\ncd Hsuanwu/\n</code></pre> After that, run the following command to install package and dependencies: <pre><code>pip install -e .[all]\n</code></pre></p>"},{"location":"contributing/#codestyle","title":"Codestyle","text":"<p>We use black codestyle (max line length of 127 characters) together with isort to sort the imports. For the documentation, we use the default line length of 88 characters per line.</p> <p>Please run <code>make format</code> to reformat your code. You can check the codestyle using make <code>check-codestyle</code> and <code>make lint</code>.</p> <p>Please document each function/method and type them using the following Google style docstring template: <pre><code>def function_with_types_in_docstring(param1: type1, param2: type2):\n    \"\"\"Example function with types documented in the docstring.\n\n    `PEP 484`_ type annotations are supported. If attribute, parameter, and\n    return types are annotated according to `PEP 484`_, they do not need to be\n    included in the docstring:\n\n    Args:\n        param1 (type1): The first parameter.\n        param2 (type2): The second parameter.\n\n    Returns:\n        bool: The return value. True for success, False otherwise.\n\n    .. _PEP 484:\n        https://www.python.org/dev/peps/pep-0484/\n\n    \"\"\"\n</code></pre></p>"},{"location":"contributing/#pull-request-pr","title":"Pull Request (PR)","text":"<p>Before proposing a PR, please open an issue, where the feature will be discussed. This prevent from duplicated PR to be proposed and also ease the code review process. Each PR need to be reviewed and accepted by at least one of the maintainers (@yuanmingqi, @ShihaoLuo). A PR must pass the Continuous Integration tests to be merged with the master branch.</p> <p>See the Pull Request Template.</p>"},{"location":"contributing/#tests","title":"Tests","text":"<p>All new features must add tests in the <code>tests/</code> folder ensuring that everything works fine. We use pytest. Also, when a bug fix is proposed, tests should be added to avoid regression.</p> <p>To run tests with <code>pytest</code>:</p> <pre><code>make pytest\n</code></pre> <p>Type checking with <code>pytype</code>:</p> <pre><code>make type\n</code></pre> <p>Codestyle check with <code>black</code>, <code>isort</code> and <code>ruff</code>:</p> <pre><code>make check-codestyle\nmake lint\n</code></pre> <p>To run <code>type</code>, <code>format</code> and <code>lint</code> in one command: <pre><code>make commit-checks\n</code></pre></p>"},{"location":"contributing/#acknowledgement","title":"Acknowledgement","text":"<p>This contributing guide is based on the stable-Baselines3 one.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Currently, Hsuanwu requires <code>Python&gt;=3.8</code>, user can create an virtual environment by <pre><code>conda create -n hsuanwu python=3.8\n</code></pre></p>"},{"location":"getting_started/#with-pip-recommended","title":"with pip recommended","text":"<p>Hsuanwu has been published as a Python package in PyPi and can be installed with <code>pip</code>, ideally by using a virtual environment. Open up a terminal and install Hsuanwu with:</p> <pre><code>pip install hsuanwu # basic installation\npip install hsuanwu[envs] # for pre-defined environments\npip install hsuanwu[tests] # for project tests\npip install hsuanwu[all] # install all the dependencies\n</code></pre>"},{"location":"getting_started/#with-git","title":"with git","text":"<p>Open up a terminal and clone the repository from GitHub witg <code>git</code>: <pre><code>git clone https://github.com/RLE-Foundation/Hsuanwu.git\n</code></pre> After that, run the following command to install package and dependencies: <pre><code>pip install -e . # basic installation\npip install -e .[envs] # for pre-defined environments\npip install -e .[tests] # for project tests\npip install -e .[all] # install all the dependencies\n</code></pre></p>"},{"location":"getting_started/#pytorch-installation","title":"PyTorch Installation","text":"<p>Hsuanwu currently supports two kinds of computing devices for acceleration, namely NVIDIA GPU and HUAWEI NPU. Thus users need to install different versions PyTorch for adapting to different devices.</p>"},{"location":"getting_started/#with-nvidia-gpu","title":"with NVIDIA GPU","text":"<p>Open up a terminal and install PyTorch with: <pre><code>pip3 install torch torchvision\n</code></pre> More information can be found in Get Started.</p> <p>Info</p> <p>Hsuanwu now supports PyTorch 2.0.0!</p>"},{"location":"getting_started/#with-huawei-npu","title":"with HUAWEI NPU","text":"<p>Tip</p> <p>Ascend NPU only supports aarch64 and Python 3.7!</p> <ul> <li>Install the dependencies for PyTorch: <pre><code>pip3 install pyyaml wheel\n</code></pre></li> <li> <p>Download the <code>.whl</code> package of PyTorch from Kunpeng file sharing center and install it: <pre><code>wget https://repo.huaweicloud.com/kunpeng/archive/Ascend/PyTorch/torch-1.11.0-cp37-cp37m-linux_aarch64.whl\npip3 install torch-1.11.0-cp37-cp37m-linux_aarch64.whl\n</code></pre></p> </li> <li> <p>Install <code>torch_npu</code>: <pre><code>wget https://gitee.com/ascend/pytorch/releases/download/v3.0.0-pytorch1.11.0/torch_npu-1.11.0rc2-cp37-cp37m-linux_aarch64.whl\npip3 install torch_npu-1.11.0rc2-cp37-cp37m-linux_aarch64.whl\n</code></pre></p> </li> <li> <p>Install <code>apex</code> [Optional]: <pre><code>wget https://gitee.com/ascend/apex/releases/download/v3.0.0-1.11.0/apex-0.1_ascend-cp37-cp37m-linux_aarch64.whl\npip3 install apex-0.1_ascend-cp37-cp37m-linux_aarch64.whl\n</code></pre> Training with mixed precision can improve the model performance. You can introduce the Apex mixed precision module or use the AMP module integrated in AscendPyTorch 1.8.1 or later based on the scenario. The Apex module provides four function modes to suit different training with mixed precision scenarios. AMP is only similar to one function of the Apex module, but can be directly used without being introduced. For details about how to use the AMP and Apex modules, see \"Mixed Precision Description\" in the PyTorch Network Model Porting and Training Guide.</p> </li> </ul>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 Reinforcement Learning Evolution Foundation</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"verification/","title":"Verification","text":""},{"location":"verification/#software","title":"Software","text":"<p>To ensure that PyTorch was installed correctly, we can verify the installation by running a single training script: <pre><code>python -m hsuanwu.verification\n</code></pre></p>"},{"location":"verification/#hardware","title":"Hardware","text":"<p>Additionally, to check if your GPU driver and CUDA is enabled and accessible by PyTorch, run the following commands to return whether or not the CUDA driver is enabled: <pre><code>import torch\ntorch.cuda.is_available()\n</code></pre></p> <p>For HUAWEI NPU:</p> <pre><code>import torch\nimport torch_npu\ntorch.npu.is_available()\n</code></pre>"},{"location":"api_docs/common/logger/","title":"Logger","text":""},{"location":"api_docs/common/logger/#logger","title":"Logger","text":"<p>source <pre><code>Logger(\nlog_dir: Path\n)\n</code></pre></p> <p>The logger class.</p> <p>Args</p> <ul> <li>log_dir  : The logging location.</li> </ul> <p>Returns</p> <p>Logger instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/logger/#parse_train_msg","title":".parse_train_msg","text":"<p>source <pre><code>.parse_train_msg(\nmsg: Any\n)\n</code></pre></p>"},{"location":"api_docs/common/logger/#parse_test_msg","title":".parse_test_msg","text":"<p>source <pre><code>.parse_test_msg(\nmsg: Any\n)\n</code></pre></p>"},{"location":"api_docs/common/logger/#time_stamp","title":".time_stamp","text":"<p>source <pre><code>.time_stamp()\n</code></pre></p> <p>Return the current time stamp.</p>"},{"location":"api_docs/common/logger/#info","title":".info","text":"<p>source <pre><code>.info(\nmsg: str\n)\n</code></pre></p> <p>Output msg with 'info' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#debug","title":".debug","text":"<p>source <pre><code>.debug(\nmsg: str\n)\n</code></pre></p> <p>Output msg with 'debug' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#error","title":".error","text":"<p>source <pre><code>.error(\nmsg: str\n)\n</code></pre></p> <p>Output msg with 'error' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#train","title":".train","text":"<p>source <pre><code>.train(\nmsg: Dict\n)\n</code></pre></p> <p>Output msg with 'train' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/logger/#test","title":".test","text":"<p>source <pre><code>.test(\nmsg: Dict\n)\n</code></pre></p> <p>Output msg with 'test' level.</p> <p>Args</p> <ul> <li>msg (str) : Message to be printed.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/timer/","title":"Timer","text":""},{"location":"api_docs/common/timer/#timer","title":"Timer","text":"<p>source </p> <p>The calculagraph class.</p> <p>Methods:</p>"},{"location":"api_docs/common/timer/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p>"},{"location":"api_docs/common/timer/#total_time","title":".total_time","text":"<p>source <pre><code>.total_time()\n</code></pre></p>"},{"location":"api_docs/common/engine/__init__/","title":"HsuanwuEngine","text":""},{"location":"api_docs/common/engine/__init__/#hsuanwuengine","title":"HsuanwuEngine","text":"<p>source <pre><code>HsuanwuEngine(\ncfgs: omegaconf.DictConfig, train_env: gym.Env, test_env: gym.Env = None\n)\n</code></pre></p> <p>Hsuanwu RL engine.</p> <p>Args</p> <ul> <li>cfgs (DictConfig) : Dict config for configuring RL algorithms.</li> <li>train_env (Env) : A Gym-like environment for training.</li> <li>test_env (Env) : A Gym-like environment for testing.</li> </ul> <p>Returns</p> <p>Hsuanwu engine instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/engine/__init__/#invoke","title":".invoke","text":"<p>source <pre><code>.invoke()\n</code></pre></p> <p>Invoke the engine to perform training.</p>"},{"location":"api_docs/common/engine/base_policy_trainer/","title":"BasePolicyTrainer","text":""},{"location":"api_docs/common/engine/base_policy_trainer/#basepolicytrainer","title":"BasePolicyTrainer","text":"<p>source <pre><code>BasePolicyTrainer(\ncfgs: omegaconf.DictConfig, train_env: gym.Env, test_env: gym.Env = None\n)\n</code></pre></p> <p>Base class of policy trainer.</p> <p>Args</p> <ul> <li>cfgs (DictConfig) : Dict config for configuring RL algorithms.</li> <li>train_env (Env) : A Gym-like environment for training.</li> <li>test_env (Env) : A Gym-like environment for testing.</li> </ul> <p>Returns</p> <p>Base policy trainer instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/engine/base_policy_trainer/#global_step","title":".global_step","text":"<p>source <pre><code>.global_step()\n</code></pre></p> <p>Get global training steps.</p>"},{"location":"api_docs/common/engine/base_policy_trainer/#global_episode","title":".global_episode","text":"<p>source <pre><code>.global_episode()\n</code></pre></p> <p>Get global training episodes.</p>"},{"location":"api_docs/common/engine/base_policy_trainer/#train","title":".train","text":"<p>source <pre><code>.train()\n</code></pre></p> <p>Training function.</p>"},{"location":"api_docs/common/engine/base_policy_trainer/#test","title":".test","text":"<p>source <pre><code>.test()\n</code></pre></p> <p>Testing function.</p>"},{"location":"api_docs/common/engine/base_policy_trainer/#save","title":".save","text":"<p>source <pre><code>.save()\n</code></pre></p> <p>Save the trained model.</p>"},{"location":"api_docs/common/engine/distributed_trainer/","title":"DistributedTrainer","text":""},{"location":"api_docs/common/engine/distributed_trainer/#distributedtrainer","title":"DistributedTrainer","text":"<p>source <pre><code>DistributedTrainer(\ncfgs: omegaconf.DictConfig, train_env: gym.Env, test_env: gym.Env = None\n)\n</code></pre></p> <p>Trainer for distributed algorithms.</p> <p>Args</p> <ul> <li>train_env (Env) : A list of Gym-like environments for training.</li> <li>test_env (Env) : A Gym-like environment for testing.</li> <li>cfgs (DictConfig) : Dict config for configuring RL algorithms.</li> </ul> <p>Returns</p> <p>Distributed trainer instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/engine/distributed_trainer/#act","title":".act","text":"<p>source <pre><code>.act(\ncfgs: omegaconf.DictConfig, logger: Logger, gym_env: gym.Env, actor_idx: int,\nactor_model: nn.Module, free_queue: mp.SimpleQueue, full_queue: mp.SimpleQueue,\nstorages: Dict[str, List], init_actor_state_storages: List[th.Tensor]\n)\n</code></pre></p> <p>Sampling function for each actor.</p> <p>Args</p> <ul> <li>cfgs (DictConfig) : Training configs.</li> <li>logger (Logger) : Hsuanwu logger.</li> <li>gym_env (Env) : A Gym-like environment.</li> <li>actor_idx (int) : The index of actor.</li> <li>actor_model (NNMoudle) : Actor network.</li> <li>free_queue (Queue) : Free queue for communication.</li> <li>full_queue (Queue) : Full queue for communication.</li> <li>storages (List[Storage]) : A list of shared storages.</li> <li>init_actor_state_storages (List[Tensor]) : Initial states for LSTM.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/common/engine/distributed_trainer/#train","title":".train","text":"<p>source <pre><code>.train()\n</code></pre></p> <p>Training function</p>"},{"location":"api_docs/common/engine/distributed_trainer/#test","title":".test","text":"<p>source <pre><code>.test()\n</code></pre></p> <p>Testing function.</p>"},{"location":"api_docs/common/engine/distributed_trainer/#save","title":".save","text":"<p>source <pre><code>.save()\n</code></pre></p> <p>Save the trained model.</p>"},{"location":"api_docs/common/engine/off_policy_trainer/","title":"OffPolicyTrainer","text":""},{"location":"api_docs/common/engine/off_policy_trainer/#offpolicytrainer","title":"OffPolicyTrainer","text":"<p>source <pre><code>OffPolicyTrainer(\ncfgs: omegaconf.DictConfig, train_env: gym.Env, test_env: gym.Env = None\n)\n</code></pre></p> <p>Trainer for off-policy algorithms.</p> <p>Args</p> <ul> <li>cfgs (DictConfig) : Dict config for configuring RL algorithms.</li> <li>train_env (Env) : A Gym-like environment for training.</li> <li>test_env (Env) : A Gym-like environment for testing.</li> </ul> <p>Returns</p> <p>Off-policy trainer instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/engine/off_policy_trainer/#train","title":".train","text":"<p>source <pre><code>.train()\n</code></pre></p> <p>Training function.</p>"},{"location":"api_docs/common/engine/off_policy_trainer/#test","title":".test","text":"<p>source <pre><code>.test()\n</code></pre></p> <p>Testing function.</p>"},{"location":"api_docs/common/engine/off_policy_trainer/#save","title":".save","text":"<p>source <pre><code>.save()\n</code></pre></p> <p>Save the trained model.</p>"},{"location":"api_docs/common/engine/on_policy_trainer/","title":"OnPolicyTrainer","text":""},{"location":"api_docs/common/engine/on_policy_trainer/#onpolicytrainer","title":"OnPolicyTrainer","text":"<p>source <pre><code>OnPolicyTrainer(\ncfgs: omegaconf.DictConfig, train_env: gym.Env, test_env: gym.Env = None\n)\n</code></pre></p> <p>Trainer for on-policy algorithms.</p> <p>Args</p> <ul> <li>cfgs (DictConfig) : Dict config for configuring RL algorithms.</li> <li>train_env (Env) : A Gym-like environment for training.</li> <li>test_env (Env) : A Gym-like environment for testing.</li> </ul> <p>Returns</p> <p>On-policy trainer instance.</p> <p>Methods:</p>"},{"location":"api_docs/common/engine/on_policy_trainer/#train","title":".train","text":"<p>source <pre><code>.train()\n</code></pre></p> <p>Training function.</p>"},{"location":"api_docs/common/engine/on_policy_trainer/#test","title":".test","text":"<p>source <pre><code>.test()\n</code></pre></p> <p>Testing function.</p>"},{"location":"api_docs/common/engine/on_policy_trainer/#save","title":".save","text":"<p>source <pre><code>.save()\n</code></pre></p> <p>Save the trained model.</p>"},{"location":"api_docs/env/utils/","title":"HsuanwuEnvWrapper","text":""},{"location":"api_docs/env/utils/#hsuanwuenvwrapper","title":"HsuanwuEnvWrapper","text":"<p>source <pre><code>HsuanwuEnvWrapper(\nenv: VectorEnv, device: str\n)\n</code></pre></p> <p>Env wrapper for adapting to Hsuanwu engine and outputting torch tensors.</p> <p>Args</p> <ul> <li>env (VectorEnv) : The vectorized environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> </ul> <p>Returns</p> <p>HsuanwuEnvWrapper instance.</p> <p>Methods:</p>"},{"location":"api_docs/env/utils/#reset","title":".reset","text":"<p>source <pre><code>.reset(\nseed: Optional[Union[int, List[int]]] = None, options: Optional[dict] = None\n)\n</code></pre></p> <p>Reset all parallel environments and return a batch of initial observations and info.</p> <p>Args</p> <ul> <li>seed (int) : The environment reset seeds.</li> <li>options (Optional[dict]) : If to return the options.</li> </ul> <p>Returns</p> <p>A batch of observations and info from the vectorized environment.</p>"},{"location":"api_docs/env/utils/#step","title":".step","text":"<p>source <pre><code>.step(\nactions: th.Tensor\n)\n</code></pre></p> <p>Take an action for each parallel environment.</p> <p>Args</p> <ul> <li>actions (Tensor) : element of :attr:<code>action_space</code> Batch of actions.</li> </ul> <p>Returns</p> <p>Batch of (observations, rewards, terminations, truncations, infos)</p>"},{"location":"api_docs/env/atari/__init__/","title":"make_atari_env","text":""},{"location":"api_docs/env/atari/__init__/#make_atari_env","title":"make_atari_env","text":"<p>source <pre><code>.make_atari_env(\nenv_id: str = 'Alien-v5', num_envs: int = 8, device: str = 'cpu', seed: int = 1,\nframe_stack: int = 4\n)\n</code></pre></p> <p>Build Atari environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of parallel environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>seed (int) : Random seed.</li> <li>frame_stack (int) : Number of stacked frames.</li> </ul> <p>Returns</p> <p>Environments instance.</p>"},{"location":"api_docs/env/bullet/__init__/","title":"make_bullet_env","text":""},{"location":"api_docs/env/bullet/__init__/#make_bullet_env","title":"make_bullet_env","text":"<p>source <pre><code>.make_bullet_env(\nenv_id: str = 'AntBulletEnv-v0', num_envs: int = 1, device: str = 'cpu', seed: int = 0\n)\n</code></pre></p> <p>Build PyBullet robotics environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of parallel environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>seed (int) : Random seed.</li> </ul> <p>Returns</p> <p>Environments instance.</p>"},{"location":"api_docs/env/dmc/__init__/","title":"make_dmc_env","text":""},{"location":"api_docs/env/dmc/__init__/#make_dmc_env","title":"make_dmc_env","text":"<p>source <pre><code>.make_dmc_env(\nenv_id: str = 'cartpole_balance', num_envs: int = 1, device: str = 'cpu',\nresource_files: Optional[List] = None, img_source: Optional[str] = None,\ntotal_frames: Optional[int] = None, seed: int = 1, visualize_reward: bool = False,\nfrom_pixels: bool = True, height: int = 84, width: int = 84, camera_id: int = 0,\nframe_stack: int = 3, frame_skip: int = 2, episode_length: int = 1000,\nenvironment_kwargs: Optional[Dict] = None\n)\n</code></pre></p> <p>Build DeepMind Control Suite environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of parallel environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>resource_files (Optional[List]) : File path of the resource files.</li> <li>img_source (Optional[str]) : Type of the background distractor, supported values: ['color', 'noise', 'images', 'video'].</li> <li>total_frames (Optional[int]) : for 'images' or 'video' distractor.</li> <li>seed (int) : Random seed.</li> <li>visualize_reward (bool) : True when 'from_pixels' is False, False when 'from_pixels' is True.</li> <li>from_pixels (bool) : Provide image-based observations or not.</li> <li>height (int) : Image observation height.</li> <li>width (int) : Image observation width.</li> <li>camera_id (int) : Camera id for generating image-based observations.</li> <li>frame_stack (int) : Number of stacked frames.</li> <li>frame_skip (int) : Number of action repeat.</li> <li>episode_length (int) : Maximum length of an episode.</li> <li>environment_kwargs (Optional[Dict]) : Other environment arguments.</li> </ul> <p>Returns</p> <p>Environments instance.</p>"},{"location":"api_docs/env/minigrid/__init__/","title":"make_minigrid_env","text":""},{"location":"api_docs/env/minigrid/__init__/#make_minigrid_env","title":"make_minigrid_env","text":"<p>source <pre><code>.make_minigrid_env(\nenv_id: str = 'Alien-v5', num_envs: int = 8, fully_observable: bool = True,\nseed: int = 0, frame_stack: int = 1, device: str = 'cpu'\n)\n</code></pre></p> <p>Build MiniGrid environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of parallel environments.</li> <li>fully_observable (bool) : 'True' for using fully observable RGB image as observation.</li> <li>seed (int) : Random seed.</li> <li>frame_stack (int) : Number of stacked frames.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> </ul> <p>Returns</p> <p>Environments instance.</p>"},{"location":"api_docs/env/multibinary/__init__/","title":"make_multibinary_env","text":""},{"location":"api_docs/env/multibinary/__init__/#make_multibinary_env","title":"make_multibinary_env","text":"<p>source <pre><code>.make_multibinary_env(\nenv_id: str = 'multibinary_state', num_envs: int = 1, device: str = 'cpu', seed: int = 0\n)\n</code></pre></p> <p>Build environments with <code>MultiBinary</code> action space for testing.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of parallel environments.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>seed (int) : Random seed.</li> </ul> <p>Returns</p> <p>Environments instance.</p>"},{"location":"api_docs/env/procgen/__init__/","title":"make_procgen_env","text":""},{"location":"api_docs/env/procgen/__init__/#make_procgen_env","title":"make_procgen_env","text":"<p>source <pre><code>.make_procgen_env(\nenv_id: str = 'bigfish', num_envs: int = 64, gamma: float = 0.99, num_levels: int = 200,\nstart_level: int = 0, distribution_mode: str = 'easy', device: str = 'cpu'\n)\n</code></pre></p> <p>Build Prcogen environments.</p> <p>Args</p> <ul> <li>env_id (str) : Name of environment.</li> <li>num_envs (int) : Number of parallel environments.</li> <li>gamma (float) : A discount factor.</li> <li>num_levels (int) : The number of unique levels that can be generated.     Set to 0 to use unlimited levels.</li> <li>start_level (int) : The lowest seed that will be used to generated levels.     'start_level' and 'num_levels' fully specify the set of possible levels.</li> <li>distribution_mode (str) : What variant of the levels to use, the options are \"easy\",     \"hard\", \"extreme\", \"memory\", \"exploration\".</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> </ul> <p>Returns</p> <p>Environments instance.</p>"},{"location":"api_docs/evaluation/comparison/","title":"Comparison","text":""},{"location":"api_docs/evaluation/comparison/#comparison","title":"Comparison","text":"<p>source <pre><code>Comparison(\nscores_x: np.ndarray, scores_y: np.ndarray, get_ci: bool = False,\nmethod: str = 'percentile', reps: int = 2000, confidence_interval_size: float = 0.95,\nrandom_state: Optional[random.RandomState] = None\n)\n</code></pre></p> <p>Compare the performance between algorithms. Based on: https://github.com/google-research/rliable/blob/master/rliable/metrics.py</p> <p>Args</p> <ul> <li>scores_x (NdArray) : A matrix of size (<code>num_runs_x</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>X</code>.</li> <li>scores_y (NdArray) : A matrix of size (<code>num_runs_y</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>Y</code>.</li> <li>get_ci (bool) : Compute CIs or not.</li> <li>method (str) :  One of <code>basic</code>, <code>percentile</code>, <code>bc</code> (identical to <code>debiased</code>,     <code>bias-corrected</code>), or <code>bca</code>.</li> <li>reps (int) : Number of bootstrap replications.</li> <li>confidence_interval_size (float) : Coverage of confidence interval.</li> <li>random_state (int) : If specified, ensures reproducibility in uncertainty estimates.</li> </ul> <p>Returns</p> <p>Comparer instance.</p> <p>Methods:</p>"},{"location":"api_docs/evaluation/comparison/#compute_poi","title":".compute_poi","text":"<p>source <pre><code>.compute_poi()\n</code></pre></p> <p>Compute the overall probability of imporvement of algorithm <code>X</code> over <code>Y</code>.</p>"},{"location":"api_docs/evaluation/comparison/#get_interval_estimates","title":".get_interval_estimates","text":"<p>source <pre><code>.get_interval_estimates(\nscores_x: np.array, scores_y: np.array, metric: Callable\n)\n</code></pre></p> <p>Computes interval estimation of the above performance evaluators.</p> <p>Args</p> <ul> <li>scores_x (NdArray) : A matrix of size (<code>num_runs_x</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>X</code>.</li> <li>scores_y (NdArray) : A matrix of size (<code>num_runs_y</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code> for algorithm <code>Y</code>.</li> <li>metric (Callable) : One of the above performance evaluators used for estimation.</li> </ul> <p>Returns</p> <p>Confidence intervals.</p>"},{"location":"api_docs/evaluation/performance/","title":"Performance","text":""},{"location":"api_docs/evaluation/performance/#performance","title":"Performance","text":"<p>source <pre><code>Performance(\nscores: np.ndarray, get_ci: bool = False, method: str = 'percentile',\ntask_bootstrap: bool = False, reps: int = 50000,\nconfidence_interval_size: float = 0.95,\nrandom_state: Optional[random.RandomState] = None\n)\n</code></pre></p> <p>Evaluate the performance of an algorithm. Based on: https://github.com/google-research/rliable/blob/master/rliable/metrics.py</p> <p>Args</p> <ul> <li>scores (NdArray) : A matrix of size (<code>num_runs</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code>.</li> <li>get_ci (bool) : Compute CIs or not.</li> <li>method (str) :  One of <code>basic</code>, <code>percentile</code>, <code>bc</code> (identical to <code>debiased</code>,     <code>bias-corrected</code>), or <code>bca</code>.</li> <li>task_bootstrap (bool) :  Whether to perform bootstrapping over tasks in addition to     runs. Defaults to False. See <code>StratifiedBoostrap</code> for more details.</li> <li>reps (int) : Number of bootstrap replications.</li> <li>confidence_interval_size (float) : Coverage of confidence interval.</li> <li>random_state (int) : If specified, ensures reproducibility in uncertainty estimates.</li> </ul> <p>Returns</p> <p>Performance evaluator.</p> <p>Methods:</p>"},{"location":"api_docs/evaluation/performance/#aggregate_mean","title":".aggregate_mean","text":"<p>source <pre><code>.aggregate_mean()\n</code></pre></p> <p>Computes mean of sample mean scores per task.</p>"},{"location":"api_docs/evaluation/performance/#aggregate_median","title":".aggregate_median","text":"<p>source <pre><code>.aggregate_median()\n</code></pre></p> <p>Computes median of sample mean scores per task.</p>"},{"location":"api_docs/evaluation/performance/#aggregate_og","title":".aggregate_og","text":"<p>source <pre><code>.aggregate_og(\ngamma: float = 1.0\n)\n</code></pre></p> <p>Computes optimality gap across all runs and tasks.</p> <p>Args</p> <ul> <li>gamma (float) : Threshold for optimality gap. All scores above <code>gamma</code> are clipped to <code>gamma</code>.</li> </ul> <p>Returns</p> <p>Optimality gap at threshold <code>gamma</code>.</p>"},{"location":"api_docs/evaluation/performance/#aggregate_iqm","title":".aggregate_iqm","text":"<p>source <pre><code>.aggregate_iqm()\n</code></pre></p> <p>Computes the interquartile mean across runs and tasks.</p>"},{"location":"api_docs/evaluation/performance/#get_interval_estimates","title":".get_interval_estimates","text":"<p>source <pre><code>.get_interval_estimates(\nscores: np.array, metric: Callable\n)\n</code></pre></p> <p>Computes interval estimation of the above performance evaluators.</p> <p>Args</p> <ul> <li>scores (NdArray) : A matrix of size (<code>num_runs</code> x <code>num_tasks</code>) where scores[n][m]     represent the score on run <code>n</code> of task <code>m</code>.</li> <li>metric (Callable) : One of the above performance evaluators used for estimation.</li> </ul> <p>Returns</p> <p>Confidence intervals.</p>"},{"location":"api_docs/evaluation/utils/","title":"Utils","text":""},{"location":"api_docs/evaluation/utils/#min_max_normalize","title":"min_max_normalize","text":"<p>source <pre><code>.min_max_normalize(\nvalue: np.ndarray, min_scores: np.ndarray, max_scores: np.ndarray\n)\n</code></pre></p> <p>Perform <code>Max-Min</code> normalization.</p>"},{"location":"api_docs/xploit/agent/base/","title":"BaseAgent","text":""},{"location":"api_docs/xploit/agent/base/#baseagent","title":"BaseAgent","text":"<p>source <pre><code>BaseAgent(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str, feature_dim: int, lr: float, eps: float\n)\n</code></pre></p> <p>Base class of agent.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> </ul> <p>Returns</p> <p>Base agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/agent/base/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining: bool = True\n)\n</code></pre></p> <p>Set the train mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (testing).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/base/#integrate","title":".integrate","text":"<p>source <pre><code>.integrate(\n**kwargs\n)\n</code></pre></p> <p>Integrate agent and other modules (encoder, reward, ...) together</p>"},{"location":"api_docs/xploit/agent/base/#act","title":".act","text":"<p>source <pre><code>.act(\nobs: th.Tensor, training: bool = True, step: int = 0\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>training (bool) : training mode, True or False.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/agent/base/#update","title":".update","text":"<p>source <pre><code>.update(\n**kwargs\n)\n</code></pre></p> <p>Update agent and return training metrics such as loss functions.</p>"},{"location":"api_docs/xploit/agent/base/#save","title":".save","text":"<p>source <pre><code>.save(\npath: Path\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (path) : Storage path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/base/#load","title":".load","text":"<p>source <pre><code>.load(\npath: Path\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (path) : Import path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/daac/","title":"DAAC","text":""},{"location":"api_docs/xploit/agent/daac/#daac","title":"DAAC","text":"<p>source <pre><code>DAAC(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str, feature_dim: int, lr: float, eps: float,\nhidden_dim: int, clip_range: float, policy_epochs: int, value_freq: int,\nvalue_epochs: int, num_mini_batch: int, vf_coef: float, ent_coef: float,\naug_coef: float, adv_coef: float, max_grad_norm: float\n)\n</code></pre></p> <p>Decoupled Advantage Actor-Critic (DAAC) agent. When 'augmentation' module is invoked, this learner will transform into Data Regularized Decoupled Actor-Critic (DrAAC) agent. Based on: https://github.com/rraileanu/idaac</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>policy_epochs (int) : Times of updating the policy network.</li> <li>value_freq (int) : Update frequency of the value network.</li> <li>value_epochs (int) : Times of updating the value network.</li> <li>num_mini_batch (int) : Number of mini-batches.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>aug_coef (float) : Weighting coefficient of augmentation loss.</li> <li>adv_ceof (float) : Weighting coefficient of advantage loss.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> </ul> <p>Returns</p> <p>DAAC learner instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/agent/daac/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining: bool = True\n)\n</code></pre></p> <p>Set the train mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (testing).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/daac/#integrate","title":".integrate","text":"<p>source <pre><code>.integrate(\n**kwargs\n)\n</code></pre></p> <p>Integrate agent and other modules (encoder, reward, ...) together</p>"},{"location":"api_docs/xploit/agent/daac/#get_value","title":".get_value","text":"<p>source <pre><code>.get_value(\nobs: th.Tensor\n)\n</code></pre></p> <p>Get estimated values for observations.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> </ul> <p>Returns</p> <p>Estimated values.</p>"},{"location":"api_docs/xploit/agent/daac/#act","title":".act","text":"<p>source <pre><code>.act(\nobs: th.Tensor, training: bool = True, step: int = 0\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs  : Observations.</li> <li>training  : training mode, True or False.</li> <li>step  : Global training step.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/agent/daac/#update","title":".update","text":"<p>source <pre><code>.update(\nrollout_storage: Storage, episode: int = 0\n)\n</code></pre></p> <p>Update the learner.</p> <p>Args</p> <ul> <li>rollout_storage (Storage) : Hsuanwu rollout storage.</li> <li>episode (int) : Global training episode.</li> </ul> <p>Returns</p> <p>Training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/xploit/agent/daac/#save","title":".save","text":"<p>source <pre><code>.save(\npath: Path\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Storage path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/daac/#load","title":".load","text":"<p>source <pre><code>.load(\npath: str\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/drqv2/","title":"DDPG","text":""},{"location":"api_docs/xploit/agent/drqv2/#drqv2","title":"DrQv2","text":"<p>source <pre><code>DrQv2(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str, feature_dim: int, lr: float, eps: float,\nhidden_dim: int, critic_target_tau: float, update_every_steps: int\n)\n</code></pre></p> <p>Data Regularized-Q v2 (DrQ-v2). When 'augmentation' module is deprecated, this agent will transform into     Deep Deterministic Policy Gradient (DDPG) agent. Based on: https://github.com/facebookresearch/drqv2/blob/main/drqv2.py</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>critic_target_tau  : The critic Q-function soft-update rate.</li> <li>update_every_steps (int) : The agent update frequency.</li> </ul> <p>Returns</p> <p>DrQv2 agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/agent/drqv2/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining: bool = True\n)\n</code></pre></p> <p>Set the train mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (testing).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/drqv2/#integrate","title":".integrate","text":"<p>source <pre><code>.integrate(\n**kwargs\n)\n</code></pre></p> <p>Integrate agent and other modules (encoder, reward, ...) together</p>"},{"location":"api_docs/xploit/agent/drqv2/#act","title":".act","text":"<p>source <pre><code>.act(\nobs: th.Tensor, training: bool = True, step: int = 0\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>training (bool) : training mode, True or False.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/agent/drqv2/#update","title":".update","text":"<p>source <pre><code>.update(\nreplay_storage, step: int = 0\n)\n</code></pre></p> <p>Update the agent.</p> <p>Args</p> <ul> <li>replay_storage (Storage) : Hsuanwu replay storage.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/xploit/agent/drqv2/#update_critic","title":".update_critic","text":"<p>source <pre><code>.update_critic(\nobs: th.Tensor, action: th.Tensor, reward: th.Tensor, discount: th.Tensor,\nnext_obs: th.Tensor, step: int\n)\n</code></pre></p> <p>Update the critic network.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>action (Tensor) : Actions.</li> <li>reward (Tensor) : Rewards.</li> <li>discount (Tensor) : discounts.</li> <li>next_obs (Tensor) : Next observations.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Critic loss metrics.</p>"},{"location":"api_docs/xploit/agent/drqv2/#update_actor","title":".update_actor","text":"<p>source <pre><code>.update_actor(\nobs: th.Tensor, step: int\n)\n</code></pre></p> <p>Update the actor network.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Actor loss metrics.</p>"},{"location":"api_docs/xploit/agent/drqv2/#save","title":".save","text":"<p>source <pre><code>.save(\npath: Path\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Storage path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/drqv2/#load","title":".load","text":"<p>source <pre><code>.load(\npath: str\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/impala/","title":"IMPALA","text":""},{"location":"api_docs/xploit/agent/impala/#impala","title":"IMPALA","text":"<p>source <pre><code>IMPALA(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str, feature_dim: int, lr: float, eps: float, use_lstm: bool,\nent_coef: float, baseline_coef: float, max_grad_norm: float, discount: float\n)\n</code></pre></p> <p>Importance Weighted Actor-Learner Architecture (IMPALA).</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>use_lstm (bool) : Use LSTM in the policy network or not.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>baseline_coef (float) : .</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>discount (float) : Discount factor.</li> </ul> <p>Returns</p> <p>IMPALA distance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/agent/impala/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining: bool = True\n)\n</code></pre></p> <p>Set the train mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (testing).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/impala/#integrate","title":".integrate","text":"<p>source <pre><code>.integrate(\n**kwargs\n)\n</code></pre></p> <p>Integrate agent and other modules (encoder, reward, ...) together</p>"},{"location":"api_docs/xploit/agent/impala/#act","title":".act","text":"<p>source <pre><code>.act(\n*kwargs\n)\n</code></pre></p> <p>Sample actions based on observations.</p>"},{"location":"api_docs/xploit/agent/impala/#update","title":".update","text":"<p>source <pre><code>.update(\ncfgs: omegaconf.DictConfig, actor_model: nn.Module, learner_model: nn.Module,\nbatch: Dict, init_actor_states: Tuple[th.Tensor, ...],\noptimizer: th.optim.Optimizer, lr_scheduler: th.optim.lr_scheduler,\nlock = threading.Lock()\n)\n</code></pre></p> <p>Update the learner model.</p> <p>Args</p> <ul> <li>cfgs (DictConfig) : Training configs.</li> <li>actor_model (NNMoudle) : Actor network.</li> <li>learner_model (NNMoudle) : Learner network.</li> <li>batch (Batch) : Batch samples.</li> <li>init_actor_states (List[Tensor]) : Initial states for LSTM.</li> <li>optimizer (th.optim.Optimizer) : Optimizer.</li> <li>lr_scheduler (th.optim.lr_scheduler) : Learning rate scheduler.</li> <li>lock (Lock) : Thread lock.</li> </ul> <p>Returns</p> <p>Training metrics.</p>"},{"location":"api_docs/xploit/agent/impala/#save","title":".save","text":"<p>source <pre><code>.save(\npath: Path\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Storage path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/impala/#load","title":".load","text":"<p>source <pre><code>.load(\npath: str\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/ppg/","title":"PPG","text":""},{"location":"api_docs/xploit/agent/ppg/#ppg","title":"PPG","text":"<p>source <pre><code>PPG(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str, feature_dim: int = 256, lr: float = 0.0005,\neps: float = 1e-05, hidden_dim: int = 256, clip_range: float = 0.2,\nnum_policy_mini_batch: int = 8, num_aux_mini_batch: int = 4, vf_coef: float = 0.5,\nent_coef: float = 0.01, aug_coef: float = 0.1, max_grad_norm: float = 0.5,\npolicy_epochs: int = 32, aux_epochs: int = 6, kl_coef: float = 1.0,\nnum_aux_grad_accum: int = 1\n)\n</code></pre></p> <p>Phasic Policy Gradient (PPG) agent. Based on: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppg_procgen.py</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>num_policy_mini_batch (int) : Number of mini-batches in policy phase.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>aug_coef (float) : Weighting coefficient of augmentation loss.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> <li>policy_epochs (int) : Number of iterations in the policy phase.</li> <li>aux_epochs (int) : Number of iterations in the auxiliary phase.</li> <li>kl_coef (float) : Weighting coefficient of divergence loss.</li> <li>num_aux_grad_accum (int) : Number of gradient accumulation for auxiliary phase update.</li> </ul> <p>num_aux_mini_batch (int) Number of mini-batches in auxiliary phase.</p> <p>Returns</p> <p>PPG agent instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/agent/ppg/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining: bool = True\n)\n</code></pre></p> <p>Set the train mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (testing).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/ppg/#integrate","title":".integrate","text":"<p>source <pre><code>.integrate(\n**kwargs\n)\n</code></pre></p> <p>Integrate agent and other modules (encoder, reward, ...) together</p>"},{"location":"api_docs/xploit/agent/ppg/#get_value","title":".get_value","text":"<p>source <pre><code>.get_value(\nobs: th.Tensor\n)\n</code></pre></p> <p>Get estimated values for observations.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> </ul> <p>Returns</p> <p>Estimated values.</p>"},{"location":"api_docs/xploit/agent/ppg/#act","title":".act","text":"<p>source <pre><code>.act(\nobs: th.Tensor, training: bool = True, step: int = 0\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs  : Observations.</li> <li>training  : training mode, True or False.</li> <li>step  : Global training step.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/agent/ppg/#update","title":".update","text":"<p>source <pre><code>.update(\nrollout_storage: Storage, episode: int = 0\n)\n</code></pre></p> <p>Update the agent.</p> <p>Args</p> <ul> <li>rollout_storage (Storage) : Hsuanwu rollout storage.</li> <li>episode (int) : Global training episode.</li> </ul> <p>Returns</p> <p>Training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/xploit/agent/ppg/#save","title":".save","text":"<p>source <pre><code>.save(\npath: Path\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Storage path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/ppg/#load","title":".load","text":"<p>source <pre><code>.load(\npath: str\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/ppo/","title":"DrAC","text":""},{"location":"api_docs/xploit/agent/ppo/#ppo","title":"PPO","text":"<p>source <pre><code>PPO(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str, feature_dim: int, lr: float, eps: float,\nhidden_dim: int, clip_range: float, n_epochs: int, num_mini_batch: int,\nvf_coef: float, ent_coef: float, aug_coef: float, max_grad_norm: float\n)\n</code></pre></p> <p>Proximal Policy Optimization (PPO) agent. When 'augmentation' module is invoked, this learner will transform into Data Regularized Actor-Critic (DrAC) agent. Based on: https://github.com/yuanmingqi/pytorch-a2c-ppo-acktr-gail</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>clip_range (float) : Clipping parameter.</li> <li>n_epochs (int) : Times of updating the policy.</li> <li>num_mini_batch (int) : Number of mini-batches.</li> <li>vf_coef (float) : Weighting coefficient of value loss.</li> <li>ent_coef (float) : Weighting coefficient of entropy bonus.</li> <li>aug_coef (float) : Weighting coefficient of augmentation loss.</li> <li>max_grad_norm (float) : Maximum norm of gradients.</li> </ul> <p>Returns</p> <p>PPO learner instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/agent/ppo/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining: bool = True\n)\n</code></pre></p> <p>Set the train mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (testing).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/ppo/#integrate","title":".integrate","text":"<p>source <pre><code>.integrate(\n**kwargs\n)\n</code></pre></p> <p>Integrate agent and other modules (encoder, reward, ...) together</p>"},{"location":"api_docs/xploit/agent/ppo/#get_value","title":".get_value","text":"<p>source <pre><code>.get_value(\nobs: th.Tensor\n)\n</code></pre></p> <p>Get estimated values for observations.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> </ul> <p>Returns</p> <p>Estimated values.</p>"},{"location":"api_docs/xploit/agent/ppo/#act","title":".act","text":"<p>source <pre><code>.act(\nobs: th.Tensor, training: bool = True, step: int = 0\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs  : Observations.</li> <li>training  : training mode, True or False.</li> <li>step  : Global training step.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/agent/ppo/#update","title":".update","text":"<p>source <pre><code>.update(\nrollout_storage: Storage, episode: int = 0\n)\n</code></pre></p> <p>Update the learner.</p> <p>Args</p> <ul> <li>rollout_storage (Storage) : Hsuanwu rollout storage.</li> <li>episode (int) : Global training episode.</li> </ul> <p>Returns</p> <p>Training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/xploit/agent/ppo/#save","title":".save","text":"<p>source <pre><code>.save(\npath: Path\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Storage path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/ppo/#load","title":".load","text":"<p>source <pre><code>.load(\npath: str\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/sac/","title":"DrQ","text":""},{"location":"api_docs/xploit/agent/sac/#sac","title":"SAC","text":"<p>source <pre><code>SAC(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str, feature_dim: int, lr: float, eps: float,\nhidden_dim: int, critic_target_tau: float, update_every_steps: int,\nlog_std_range: Tuple[float], betas: Tuple[float], temperature: float,\nfixed_temperature: bool, discount: float\n)\n</code></pre></p> <p>Soft Actor-Critic (SAC) agent. When 'augmentation' module is invoked, this learner will transform into Data Regularized Q (DrQ) agent. Based on: https://github.com/denisyarats/pytorch_sac</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>feature_dim (int) : Number of features extracted by the encoder.</li> <li>lr (float) : The learning rate.</li> <li>eps (float) : Term added to the denominator to improve numerical stability.</li> <li>hidden_dim (int) : The size of the hidden layers.</li> <li>critic_target_tau (float) : The critic Q-function soft-update rate.</li> <li>update_every_steps (int) : The agent update frequency.</li> <li>log_std_range (Tuple[float]) : Range of std for sampling actions.</li> <li>betas (Tuple[float]) : coefficients used for computing running averages of gradient and its square.</li> <li>temperature (float) : Initial temperature coefficient.</li> <li>fixed_temperature (bool) : Fixed temperature or not.</li> <li>discount (float) : Discount factor.</li> </ul> <p>Returns</p> <p>Soft Actor-Critic learner instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/agent/sac/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining: bool = True\n)\n</code></pre></p> <p>Set the train mode.</p> <p>Args</p> <ul> <li>training (bool) : True (training) or False (testing).</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/sac/#integrate","title":".integrate","text":"<p>source <pre><code>.integrate(\n**kwargs\n)\n</code></pre></p> <p>Integrate agent and other modules (encoder, reward, ...) together</p>"},{"location":"api_docs/xploit/agent/sac/#alpha","title":".alpha","text":"<p>source <pre><code>.alpha()\n</code></pre></p> <p>Get the temperature coefficient.</p>"},{"location":"api_docs/xploit/agent/sac/#act","title":".act","text":"<p>source <pre><code>.act(\nobs: th.Tensor, training: bool = True, step: int = 0\n)\n</code></pre></p> <p>Sample actions based on observations.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>training (bool) : training mode, True or False.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Sampled actions.</p>"},{"location":"api_docs/xploit/agent/sac/#update","title":".update","text":"<p>source <pre><code>.update(\nreplay_storage, step: int = 0\n)\n</code></pre></p> <p>Update the learner.</p> <p>Args</p> <ul> <li>replay_storage (Storage) : Hsuanwu replay storage.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Training metrics such as actor loss, critic_loss, etc.</p>"},{"location":"api_docs/xploit/agent/sac/#update_critic","title":".update_critic","text":"<p>source <pre><code>.update_critic(\nobs: th.Tensor, action: th.Tensor, reward: th.Tensor, terminated: th.Tensor,\nnext_obs: th.Tensor, weights: th.Tensor, aug_obs: th.Tensor,\naug_next_obs: th.Tensor, step: int\n)\n</code></pre></p> <p>Update the critic network.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>action (Tensor) : Actions.</li> <li>reward (Tensor) : Rewards.</li> <li>terminated (Tensor) : Terminateds.</li> <li>next_obs (Tensor) : Next observations.</li> <li>weights (Tensor) : Batch sample weights.</li> <li>aug_obs (Tensor) : Augmented observations.</li> <li>aug_next_obs (Tensor) : Augmented next observations.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Critic loss metrics.</p>"},{"location":"api_docs/xploit/agent/sac/#update_actor_and_alpha","title":".update_actor_and_alpha","text":"<p>source <pre><code>.update_actor_and_alpha(\nobs: th.Tensor, weights: th.Tensor, step: int\n)\n</code></pre></p> <p>Update the actor network and temperature.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>weights (Tensor) : Batch sample weights.</li> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Actor loss metrics.</p>"},{"location":"api_docs/xploit/agent/sac/#save","title":".save","text":"<p>source <pre><code>.save(\npath: Path\n)\n</code></pre></p> <p>Save models.</p> <p>Args</p> <ul> <li>path (Path) : Storage path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/agent/sac/#load","title":".load","text":"<p>source <pre><code>.load(\npath: str\n)\n</code></pre></p> <p>Load initial parameters.</p> <p>Args</p> <ul> <li>path (str) : Import path.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/encoder/base/","title":"BaseEncoder","text":""},{"location":"api_docs/xploit/encoder/base/#baseencoder","title":"BaseEncoder","text":"<p>source <pre><code>BaseEncoder(\nobservation_space: Union[gym.Space, DictConfig], feature_dim: int = 0\n)\n</code></pre></p> <p>Base class that represents a features extractor.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>The base encoder class</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/base/#feature_dim","title":".feature_dim","text":"<p>source <pre><code>.feature_dim()\n</code></pre></p>"},{"location":"api_docs/xploit/encoder/espeholt_residual_encoder/","title":"EspeholtResidualEncoder","text":""},{"location":"api_docs/xploit/encoder/espeholt_residual_encoder/#espeholtresidualencoder","title":"EspeholtResidualEncoder","text":"<p>source <pre><code>EspeholtResidualEncoder(\nobservation_space: Union[gym.Space, DictConfig], feature_dim: int = 0,\nnet_arch: List[int] = [16, 32, 32]\n)\n</code></pre></p> <p>ResNet-like encoder for processing image-based observations. Proposed by Espeholt L, Soyer H, Munos R, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures[C]//International conference on machine learning. PMLR, 2018: 1407-1416. Target task: Atari games and Procgen games.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>feature_dim (int) : Number of features extracted.</li> <li>net_arch (List) : Architecture of the network.     It represents the out channels of each residual layer.     The length of this list is the number of residual layers.</li> </ul> <p>Returns</p> <p>ResNet-like encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/espeholt_residual_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xploit/encoder/identity_encoder/","title":"IdentityEncoder","text":""},{"location":"api_docs/xploit/encoder/identity_encoder/#identityencoder","title":"IdentityEncoder","text":"<p>source <pre><code>IdentityEncoder(\nobservation_space: Union[gym.Space, DictConfig], feature_dim: int = 64\n)\n</code></pre></p> <p>Identity encoder for state-based observations.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>Identity encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/identity_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xploit/encoder/mnih_cnn_encoder/","title":"MnihCnnEncoder","text":""},{"location":"api_docs/xploit/encoder/mnih_cnn_encoder/#mnihcnnencoder","title":"MnihCnnEncoder","text":"<p>source <pre><code>MnihCnnEncoder(\nobservation_space: Union[gym.Space, DictConfig], feature_dim: int = 0\n)\n</code></pre></p> <p>Convolutional neural network (CNN)-based encoder for processing image-based observations. Proposed by Mnih V, Kavukcuoglu K, Silver D, et al. Playing atari with deep reinforcement learning[J]. arXiv preprint arXiv:1312.5602, 2013. Target task: Atari games.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>CNN-based encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/mnih_cnn_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xploit/encoder/tassa_cnn_encoder/","title":"TassaCnnEncoder","text":""},{"location":"api_docs/xploit/encoder/tassa_cnn_encoder/#tassacnnencoder","title":"TassaCnnEncoder","text":"<p>source <pre><code>TassaCnnEncoder(\nobservation_space: Union[gym.Space, DictConfig], feature_dim: int = 50\n)\n</code></pre></p> <p>Convolutional neural network (CNN)-based encoder for processing image-based observations. Proposed by Tassa Y, Doron Y, Muldal A, et al. Deepmind control suite[J]. arXiv preprint arXiv:1801.00690, 2018. Target task: DeepMind Control Suite.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>feature_dim (int) : Number of features extracted.</li> </ul> <p>Returns</p> <p>CNN-based encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/tassa_cnn_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xploit/encoder/vanilla_mlp_encoder/","title":"VanillaMlpEncoder","text":""},{"location":"api_docs/xploit/encoder/vanilla_mlp_encoder/#vanillamlpencoder","title":"VanillaMlpEncoder","text":"<p>source <pre><code>VanillaMlpEncoder(\nobservation_space: Union[gym.Space, DictConfig], feature_dim: int = 64,\nhidden_dim: int = 256\n)\n</code></pre></p> <p>Multi layer perceptron (MLP) for processing state-based inputs.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>feature_dim (int) : Number of features extracted.</li> <li>hidden_dim (int) : Number of units per hidden layer.</li> </ul> <p>Returns</p> <p>Mlp-based encoder instance.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/encoder/vanilla_mlp_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xploit/storage/base/","title":"BaseStorage","text":""},{"location":"api_docs/xploit/storage/base/#basestorage","title":"BaseStorage","text":"<p>source <pre><code>BaseStorage(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu'\n)\n</code></pre></p> <p>Base class of storage module.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> </ul> <p>Returns</p> <p>Instance of the base storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/base/#add","title":".add","text":"<p>source <pre><code>.add(\n*args\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p>"},{"location":"api_docs/xploit/storage/base/#sample","title":".sample","text":"<p>source <pre><code>.sample(\n*args\n)\n</code></pre></p> <p>Sample from the storage.</p>"},{"location":"api_docs/xploit/storage/base/#update","title":".update","text":"<p>source <pre><code>.update(\n*args\n)\n</code></pre></p> <p>Update the storage</p>"},{"location":"api_docs/xploit/storage/decoupled_rollout_storage/","title":"DecoupledRolloutStorage","text":""},{"location":"api_docs/xploit/storage/decoupled_rollout_storage/#decoupledrolloutstorage","title":"DecoupledRolloutStorage","text":"<p>source <pre><code>DecoupledRolloutStorage(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', num_steps: int = 256, num_envs: int = 8,\ndiscount: float = 0.99, gae_lambda: float = 0.95\n)\n</code></pre></p> <p>Decoupled rollout storage for on-policy algorithms like DAAC.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>discount (float) : discount factor.</li> <li>gae_lambda (float) : Weighting coefficient for generalized advantage estimation (GAE).</li> </ul> <p>Returns</p> <p>Vanilla rollout storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/decoupled_rollout_storage/#add","title":".add","text":"<p>source <pre><code>.add(\nobs: th.Tensor, actions: th.Tensor, rewards: th.Tensor, terminateds: th.Tensor,\ntruncateds: th.Tensor, next_obs: th.Tensor, log_probs: th.Tensor,\nvalues: th.Tensor, adv_preds: th.Tensor\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>actions (Tensor) : Actions.</li> <li>rewards (Tensor) : Rewards.</li> <li>terminateds (Tensor) : Terminateds.</li> <li>truncateds (Tensor) : Truncateds.</li> <li>next_obs (Tensor) : Next observations.</li> <li>log_probs (Tensor) : Log of the probability evaluated at <code>actions</code>.</li> <li>values (Tensor) : Estimated values.</li> <li>adv_preds (Tensor) : Predicted advantages.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/decoupled_rollout_storage/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Reset the terminal state of each env.</p>"},{"location":"api_docs/xploit/storage/decoupled_rollout_storage/#compute_returns_and_advantages","title":".compute_returns_and_advantages","text":"<p>source <pre><code>.compute_returns_and_advantages(\nlast_values: th.Tensor\n)\n</code></pre></p> <p>Perform generalized advantage estimation (GAE).</p> <p>Args</p> <ul> <li>last_values (Tensor) : Estimated values of the last step.</li> <li>gamma (float) : Discount factor.</li> <li>gae_lamdba (float) : Coefficient of GAE.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/decoupled_rollout_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nnum_mini_batch: int = 8\n)\n</code></pre></p> <p>Sample data from storage.</p> <p>Args</p> <ul> <li>num_mini_batch (int) : Number of mini-batches</li> </ul> <p>Returns</p> <p>Batch data.</p>"},{"location":"api_docs/xploit/storage/distributed_storage/","title":"DistributedStorage","text":""},{"location":"api_docs/xploit/storage/distributed_storage/#distributedstorage","title":"DistributedStorage","text":"<p>source <pre><code>DistributedStorage(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', num_steps: int = 100, num_storages: int = 80,\nbatch_size: int = 32\n)\n</code></pre></p> <p>Distributed storage for distributed algorithms like IMPALA.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>num_steps (int) : The sample steps of per rollout.</li> <li>num_storages (int) : The number of shared-memory storages.</li> <li>batch_size (int) : The batch size.</li> </ul> <p>Returns</p> <p>Vanilla rollout storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/distributed_storage/#add","title":".add","text":"<p>source <pre><code>.add(\n*args\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p>"},{"location":"api_docs/xploit/storage/distributed_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\ndevice: th.device, batch_size: int, free_queue: th.multiprocessing.SimpleQueue,\nfull_queue: th.multiprocessing.SimpleQueue, storages: List,\ninit_actor_state_storages: List, lock = threading.Lock()\n)\n</code></pre></p> <p>Sample transitions from the storage.</p> <p>Args</p> <ul> <li>device (Device) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>batch_size (int) : The batch size.</li> <li>free_queue (Queue) : Free queue for communication.</li> <li>full_queue (Queue) : Full queue for communication.</li> <li>storages (List[Storage]) : A list of shared storages.</li> <li>init_actor_state_storages  : (List[Tensor]): Initial states for LSTM.</li> <li>lock (Lock) : Thread lock.</li> </ul> <p>Returns</p> <p>Batched samples.</p>"},{"location":"api_docs/xploit/storage/distributed_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n*args\n)\n</code></pre></p> <p>Update the storage</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/","title":"NStepReplayStorage","text":""},{"location":"api_docs/xploit/storage/nstep_replay_storage/#nstepreplaystorage","title":"NStepReplayStorage","text":"<p>source <pre><code>NStepReplayStorage(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', storage_size: int = 500000, batch_size: int = 256,\nnum_workers: int = 4, pin_memory: bool = True, n_step: int = 3, discount: float = 0.99,\nfetch_every: int = 1000, save_snapshot: bool = False\n)\n</code></pre></p> <p>Replay storage for off-policy algorithms (N-step returns supported).</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>storage_size (int) : Max number of element in the storage.</li> <li>batch_size (int) : Number of samples per batch to load.</li> <li>num_workers (int) : Subprocesses to use for data loading.</li> <li>pin_memory (bool) : Copy Tensors into device/CUDA pinned memory before returning them.</li> <li>discount (float) : The discount factor for future rewards.</li> <li>fetch_every (int) : Loading interval.</li> <li>save_snapshot (bool) : Save loaded file or not. n_step (int) The number of transitions to consider when computing n-step returns</li> </ul> <p>Returns</p> <p>N-step replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\nobs: Any, action: Any, reward: Any, terminated: Any, info: Any, next_obs: Any\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>obs (Any) : Observations.</li> <li>action (Any) : Actions.</li> <li>reward (Any) : Rewards.</li> <li>terminated (Any) : Terminateds.</li> <li>info (Any) : Infos.</li> <li>next_obs (Any) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#replay_iter","title":".replay_iter","text":"<p>source <pre><code>.replay_iter()\n</code></pre></p> <p>Create iterable dataloader.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nstep: int\n)\n</code></pre></p> <p>Generate samples.</p> <p>Args</p> <ul> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Batched samples.</p>"},{"location":"api_docs/xploit/storage/nstep_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n*args\n)\n</code></pre></p> <p>Update the storage</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/","title":"PrioritizedReplayStorage","text":""},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#prioritizedreplaystorage","title":"PrioritizedReplayStorage","text":"<p>source <pre><code>PrioritizedReplayStorage(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', storage_size: int = 1000000, batch_size: int = 1024,\nalpha: float = 0.6, beta: float = 0.4\n)\n</code></pre></p> <p>Prioritized replay storage with proportional prioritization for off-policy algorithms.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>storage_size (int) : Max number of element in the buffer.</li> <li>batch_size (int) : Batch size of samples.</li> <li>alpha (float) : The alpha coefficient.</li> <li>beta (float) : The beta coefficient.</li> </ul> <p>Returns</p> <p>Prioritized replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#annealing_beta","title":".annealing_beta","text":"<p>source <pre><code>.annealing_beta(\nstep: int\n)\n</code></pre></p> <p>Linearly increases beta from the initial value to 1 over global training steps.</p> <p>Args</p> <ul> <li>step (int) : The global training step.</li> </ul> <p>Returns</p> <p>Beta value.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\nobs: Any, action: Any, reward: Any, terminated: Any, info: Any, next_obs: Any\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>obs (Any) : Observations.</li> <li>action (Any) : Actions.</li> <li>reward (Any) : Rewards.</li> <li>terminated (Any) : Terminateds.</li> <li>info (Any) : Infos.</li> <li>next_obs (Any) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nstep: int\n)\n</code></pre></p> <p>Sample from the storage.</p> <p>Args</p> <ul> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Batched samples.</p>"},{"location":"api_docs/xploit/storage/prioritized_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\nmetrics: Dict\n)\n</code></pre></p> <p>Update the priorities.</p> <p>Args</p> <ul> <li>metrics (Dict) : Training metrics from agent to udpate the priorities:     indices (NdArray): The indices of current batch data.     priorities (NdArray): The priorities of current batch data.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/","title":"VanillaReplayStorage","text":""},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#vanillareplaystorage","title":"VanillaReplayStorage","text":"<p>source <pre><code>VanillaReplayStorage(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', storage_size: int = 1000000, batch_size: int = 1024\n)\n</code></pre></p> <p>Vanilla replay storage for off-policy algorithms.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>storage_size (int) : Max number of element in the buffer.</li> <li>batch_size (int) : Batch size of samples.</li> </ul> <p>Returns</p> <p>Vanilla replay storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#add","title":".add","text":"<p>source <pre><code>.add(\nobs: Any, action: Any, reward: Any, terminated: Any, info: Any, next_obs: Any\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>obs (Any) : Observations.</li> <li>action (Any) : Actions.</li> <li>reward (Any) : Rewards.</li> <li>terminated (Any) : Terminateds.</li> <li>info (Any) : Infos.</li> <li>next_obs (Any) : Next observations.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nstep: int\n)\n</code></pre></p> <p>Sample from the storage.</p> <p>Args</p> <ul> <li>step (int) : Global training step.</li> </ul> <p>Returns</p> <p>Batched samples.</p>"},{"location":"api_docs/xploit/storage/vanilla_replay_storage/#update","title":".update","text":"<p>source <pre><code>.update(\n*args\n)\n</code></pre></p> <p>Update the storage</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/","title":"VanillaRolloutStorage","text":""},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#vanillarolloutstorage","title":"VanillaRolloutStorage","text":"<p>source <pre><code>VanillaRolloutStorage(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', num_steps: int = 256, num_envs: int = 8,\ndiscount: float = 0.99, gae_lambda: float = 0.95\n)\n</code></pre></p> <p>Vanilla rollout storage for on-policy algorithms.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>num_steps (int) : The sample length of per rollout.</li> <li>num_envs (int) : The number of parallel environments.</li> <li>discount (float) : discount factor.</li> <li>gae_lambda (float) : Weighting coefficient for generalized advantage estimation (GAE).</li> </ul> <p>Returns</p> <p>Vanilla rollout storage.</p> <p>Methods:</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#add","title":".add","text":"<p>source <pre><code>.add(\nobs: th.Tensor, actions: th.Tensor, rewards: th.Tensor, terminateds: th.Tensor,\ntruncateds: th.Tensor, next_obs: th.Tensor, log_probs: th.Tensor,\nvalues: th.Tensor\n)\n</code></pre></p> <p>Add sampled transitions into storage.</p> <p>Args</p> <ul> <li>obs (Tensor) : Observations.</li> <li>actions (Tensor) : Actions.</li> <li>rewards (Tensor) : Rewards.</li> <li>terminateds (Tensor) : Terminateds.</li> <li>truncateds (Tensor) : Truncateds.</li> <li>next_obs (Tensor) : Next observations.</li> <li>log_probs (Tensor) : Log of the probability evaluated at <code>actions</code>.</li> <li>values (Tensor) : Estimated values.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#update","title":".update","text":"<p>source <pre><code>.update()\n</code></pre></p> <p>Reset the terminal state of each env.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#compute_returns_and_advantages","title":".compute_returns_and_advantages","text":"<p>source <pre><code>.compute_returns_and_advantages(\nlast_values: th.Tensor\n)\n</code></pre></p> <p>Perform generalized advantage estimation (GAE).</p> <p>Args</p> <ul> <li>last_values (Tensor) : Estimated values of the last step.</li> <li>gamma (float) : Discount factor.</li> <li>gae_lamdba (float) : Coefficient of GAE.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xploit/storage/vanilla_rollout_storage/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nnum_mini_batch: int = 8\n)\n</code></pre></p> <p>Sample data from storage.</p> <p>Args</p> <ul> <li>num_mini_batch (int) : Number of mini-batches</li> </ul> <p>Returns</p> <p>Batch data.</p>"},{"location":"api_docs/xplore/augmentation/auto_augment/","title":"AutoAugment","text":""},{"location":"api_docs/xplore/augmentation/auto_augment/#autoaugment","title":"AutoAugment","text":"<p>source <pre><code>AutoAugment(\naugment_policy: str = T.AutoAugmentPolicy.IMAGENET\n)\n</code></pre></p> <p>Augmentation method based on \u201cAutoAugment: Learning Augmentation Strategies from Data\u201d.</p> <p>Args</p> <ul> <li>augment_policy (str) : Desired policy enum defined by torchvision.transforms.autoaugment.AutoAugmentPolicy.     Default is AutoAugmentPolicy.IMAGENET.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/auto_augment/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/base/","title":"BaseAugmentation","text":""},{"location":"api_docs/xplore/augmentation/base/#baseaugmentation","title":"BaseAugmentation","text":"<p>source </p> <p>Base class of augmentation.</p>"},{"location":"api_docs/xplore/augmentation/elastic_transform/","title":"ElasticTransform","text":""},{"location":"api_docs/xplore/augmentation/elastic_transform/#elastictransform","title":"ElasticTransform","text":"<p>source <pre><code>ElasticTransform(\nalpha: float = 50.0, sigma: float = 5.0, interpolation: int = 0, fill = 0\n)\n</code></pre></p> <p>ElasticTransform method based on \u201cElasticTransform: Transform a image with elastic transformations\u201d.</p> <p>Args</p> <ul> <li>alpha (float or sequence of python:floats) : Magnitude of displacements. Default is 50.0.</li> <li>sigma (float or sequence of python:floats) : Smoothness of displacements. Default is 5.0.</li> <li>interpolation (InterpolationMode) : Desired interpolation enum defined by torchvision.transforms.InterpolationMode.     Default is InterpolationMode.BILINEAR.</li> <li>fill (sequence or int number) : Pixel fill value for the area outside the transformed image. Default is 0.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/elastic_transform/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/gaussian_noise/","title":"GaussianNoise","text":""},{"location":"api_docs/xplore/augmentation/gaussian_noise/#gaussiannoise","title":"GaussianNoise","text":"<p>source <pre><code>GaussianNoise(\nmu: float = 0, sigma: float = 1.0\n)\n</code></pre></p> <p>Gaussian noise operation for processing state-based observations.</p> <p>Args</p> <ul> <li>mu (float or Tensor) : mean of the distribution.</li> <li>scale (float or Tensor) : standard deviation of the distribution.</li> </ul> <p>Returns</p> <p>Augmented states.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/gaussian_noise/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/grayscale/","title":"GrayScale","text":""},{"location":"api_docs/xplore/augmentation/grayscale/#grayscale","title":"GrayScale","text":"<p>source </p> <p>Grayscale operation for image augmentation.</p> <p>Args</p> <p>None.</p> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/grayscale/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_adjustsharpness/","title":"RandomAdjustSharpness","text":""},{"location":"api_docs/xplore/augmentation/random_adjustsharpness/#randomadjustsharpness","title":"RandomAdjustSharpness","text":"<p>source <pre><code>RandomAdjustSharpness(\nsharpness_factor: float = 50.0, p: float = 5.0\n)\n</code></pre></p> <p>RandomAdjustSharpness method based on \u201cRandomAdjustSharpness: Adjust the sharpness of the image randomly with a given probability\u201d.</p> <p>Args</p> <ul> <li>sharpness_factor (float) : How much to adjust the sharpness. Can be any non-negative number. Default is 2.</li> <li>p (float) : probability of the image being sharpened. Default value is 0.5</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_adjustsharpness/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_amplitude_scaling/","title":"RandomAmplitudeScaling","text":""},{"location":"api_docs/xplore/augmentation/random_amplitude_scaling/#randomamplitudescaling","title":"RandomAmplitudeScaling","text":"<p>source <pre><code>RandomAmplitudeScaling(\nlow: float = 0.6, high: float = 1.2\n)\n</code></pre></p> <p>Random amplitude scaling operation for processing state-based observations.</p> <p>Args</p> <ul> <li>low (float) : lower range (inclusive).</li> <li>high (float) : upper range (exclusive).</li> </ul> <p>Returns</p> <p>Augmented states.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_amplitude_scaling/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_augment/","title":"RandomAugment","text":""},{"location":"api_docs/xplore/augmentation/random_augment/#randomaugment","title":"RandomAugment","text":"<p>source </p> <p>Randomly augments the data</p> <p>Args</p> <p>None.</p> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_augment/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_autocontrast/","title":"RandomAutocontrast","text":""},{"location":"api_docs/xplore/augmentation/random_autocontrast/#randomautocontrast","title":"RandomAutocontrast","text":"<p>source <pre><code>RandomAutocontrast(\np: float = 0.5\n)\n</code></pre></p> <p>RandomAutocontrast method based on \u201cRandomAutocontrast: Autocontrast the pixels of the given image randomly with a given probability\u201d.</p> <p>Args</p> <ul> <li>p (float) : probability of the image being autocontrasted. Default value is 0.5</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_autocontrast/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_colorjitter/","title":"RandomColorJitter","text":""},{"location":"api_docs/xplore/augmentation/random_colorjitter/#randomcolorjitter","title":"RandomColorJitter","text":"<p>source <pre><code>RandomColorJitter(\nbrightness: float = 0.4, contrast: float = 0.4, saturation: float = 0.4,\nhue: float = 0.5\n)\n</code></pre></p> <p>Random ColorJitter operation for image augmentation.</p> <p>Args</p> <ul> <li>brightness (float) : How much to jitter brightness. Should be non negative numbers.</li> <li>contrast (float) : How much to jitter contrast. Should be non negative numbers.</li> <li>saturation (float) : How much to jitter saturation. Should be non negative numbers.</li> <li>hue (float) : How much to jitter hue. Should have 0&lt;= hue &lt;= 0.5 or -0.5 &lt;= min &lt;= max &lt;= 0.5.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_colorjitter/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_convolution/","title":"RandomConvolution","text":""},{"location":"api_docs/xplore/augmentation/random_convolution/#randomconvolution","title":"RandomConvolution","text":"<p>source </p> <p>Random Convolution operation for image augmentation. Note that imgs should be normalized and torch tensor.</p> <p>Args</p> <p>None.</p> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_convolution/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_crop/","title":"RandomCrop","text":""},{"location":"api_docs/xplore/augmentation/random_crop/#randomcrop","title":"RandomCrop","text":"<p>source <pre><code>RandomCrop(\npad: int = 4, out: int = 84\n)\n</code></pre></p> <p>Random crop operation for processing image-based observations.</p> <p>Args</p> <ul> <li>pad (int) : Padding size.</li> <li>out (int) : Desired output size.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_crop/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_cutout/","title":"RandomCutout","text":""},{"location":"api_docs/xplore/augmentation/random_cutout/#randomcutout","title":"RandomCutout","text":"<p>source <pre><code>RandomCutout(\nmin_cut: int = 10, max_cut: int = 30\n)\n</code></pre></p> <p>Random Cutout operation for image augmentation.</p> <p>Args</p> <ul> <li>min_cut (int) : Min size of the cut shape.</li> <li>max_cut (int) : Max size of the cut shape.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_cutout/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_cutoutcolor/","title":"RandomCutoutColor","text":""},{"location":"api_docs/xplore/augmentation/random_cutoutcolor/#randomcutoutcolor","title":"RandomCutoutColor","text":"<p>source <pre><code>RandomCutoutColor(\nmin_cut: int = 10, max_cut: int = 30\n)\n</code></pre></p> <p>Random Cutout operation for image augmentation.</p> <p>Args</p> <ul> <li>min_cut (int) : min size of the cut shape.</li> <li>max_cut (int) : max size of the cut shape.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_cutoutcolor/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_equalize/","title":"RandomEqualize","text":""},{"location":"api_docs/xplore/augmentation/random_equalize/#randomequalize","title":"RandomEqualize","text":"<p>source <pre><code>RandomEqualize(\np: float = 0.5\n)\n</code></pre></p> <p>RandomEqualize method based on \u201cRandomEqualize: Equalize the histogram of the given image randomly with a given probability\u201d.</p> <p>Args</p> <ul> <li>p (float) : probability of the image being equalized. Default value is 0.5</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_equalize/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_flip/","title":"RandomFlip","text":""},{"location":"api_docs/xplore/augmentation/random_flip/#randomflip","title":"RandomFlip","text":"<p>source <pre><code>RandomFlip(\np: float = 0.2\n)\n</code></pre></p> <p>Random flip operation for image augmentation.</p> <p>Args</p> <ul> <li>p (float) : The image flip problistily in a batch.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_flip/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_invert/","title":"RandomInvert","text":""},{"location":"api_docs/xplore/augmentation/random_invert/#randominvert","title":"RandomInvert","text":"<p>source <pre><code>RandomInvert(\np: float = 0.5\n)\n</code></pre></p> <p>RandomInvert method based on \u201cRandomInvert: Inverts the colors of the given image randomly with a given probability\u201d.</p> <p>Args</p> <ul> <li>p (float) : probability of the image being color inverted. Default value is 0.5</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_invert/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_perspective/","title":"RandomPerspective","text":""},{"location":"api_docs/xplore/augmentation/random_perspective/#randomperspective","title":"RandomPerspective","text":"<p>source <pre><code>RandomPerspective(\ndistortion_scale: float = 0.5, p: float = 0.5, interpolation: int = 0, fill = 0\n)\n</code></pre></p> <p>RandomPerspective method based on \u201cRandomPerspective: Performs a random perspective transformation of the given image with a given probability.\u201d.</p> <p>Args</p> <ul> <li>distortion_scale (float) : argument to control the degree of distortion and ranges from 0 to 1. Default is 0.5.</li> <li>p (float) : Smoothness of displacements. Default is 5.0.</li> <li>interpolation (Union, InterpolationMode) : Desired interpolation enum defined by     torchvision.transforms.InterpolationMode. Default is InterpolationMode.BILINEAR.</li> <li>fill (sequence or int number) : Pixel fill value for the area outside the transformed image. Default is 0.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_perspective/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_rotate/","title":"RandomRotate","text":""},{"location":"api_docs/xplore/augmentation/random_rotate/#randomrotate","title":"RandomRotate","text":"<p>source <pre><code>RandomRotate(\np: float = 0.2\n)\n</code></pre></p> <p>Random rotate operation for processing image-based observations.</p> <p>Args</p> <ul> <li>p (float) : The image rotate problistily in a batch.</li> </ul> <p>Returns</p> <p>Random rotate image in a batch.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_rotate/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_shift/","title":"RandomShift","text":""},{"location":"api_docs/xplore/augmentation/random_shift/#randomshift","title":"RandomShift","text":"<p>source <pre><code>RandomShift(\npad: int = 4\n)\n</code></pre></p> <p>Random shift operation for processing image-based observations.</p> <p>Args</p> <ul> <li>pad  : Padding size.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_shift/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/augmentation/random_translate/","title":"RandomTranslate","text":""},{"location":"api_docs/xplore/augmentation/random_translate/#randomtranslate","title":"RandomTranslate","text":"<p>source <pre><code>RandomTranslate(\nsize: int = 256, scale_factor: float = 0.75\n)\n</code></pre></p> <p>Random translate operation for processing image-based observations.</p> <p>Args</p> <ul> <li>size (int) : The scale size in translated images</li> <li>scale_factor (float) : The scale factor ratio in translated images. Should have 0.0 &lt;= scale_factor &lt;= 1.0</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/augmentation/random_translate/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: th.Tensor\n)\n</code></pre></p>"},{"location":"api_docs/xplore/distribution/base/","title":"BaseDistribution","text":""},{"location":"api_docs/xplore/distribution/base/#basedistribution","title":"BaseDistribution","text":"<p>source </p> <p>Abstract base class of distributions.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/base/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/base/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/base/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nvalue: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at <code>value</code>.</p> <p>Args</p> <ul> <li>value (Tensor) : The value to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/base/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/base/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the distribution.</p>"},{"location":"api_docs/xplore/distribution/base/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/base/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/base/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/base/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/","title":"Bernoulli","text":""},{"location":"api_docs/xplore/distribution/bernoulli/#bernoulli","title":"Bernoulli","text":"<p>source <pre><code>Bernoulli(\nlogits: th.Tensor\n)\n</code></pre></p> <p>Bernoulli distribution for sampling actions for 'MultiBinary' tasks.</p> <p>Args</p> <ul> <li>logits (Tensor) : The event log probabilities (unnormalized).</li> </ul> <p>Returns</p> <p>Categorical distribution instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#probs","title":".probs","text":"<p>source <pre><code>.probs()\n</code></pre></p> <p>Return probabilities.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#logits","title":".logits","text":"<p>source <pre><code>.logits()\n</code></pre></p> <p>Returns the unnormalized log probabilities.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (TorchSize) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nactions: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at actions.</p> <p>Args</p> <ul> <li>actions (Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the distribution.</p>"},{"location":"api_docs/xplore/distribution/bernoulli/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = ...\n)\n</code></pre></p>"},{"location":"api_docs/xplore/distribution/categorical/","title":"Categorical","text":""},{"location":"api_docs/xplore/distribution/categorical/#categorical","title":"Categorical","text":"<p>source <pre><code>Categorical(\nlogits: th.Tensor\n)\n</code></pre></p> <p>Categorical distribution for sampling actions for 'Discrete' tasks.</p> <p>Args</p> <ul> <li>logits (Tensor) : The event log probabilities (unnormalized).</li> </ul> <p>Returns</p> <p>Categorical distribution instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/categorical/#probs","title":".probs","text":"<p>source <pre><code>.probs()\n</code></pre></p> <p>Return probabilities.</p>"},{"location":"api_docs/xplore/distribution/categorical/#logits","title":".logits","text":"<p>source <pre><code>.logits()\n</code></pre></p> <p>Returns the unnormalized log probabilities.</p>"},{"location":"api_docs/xplore/distribution/categorical/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (TorchSize) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/categorical/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nactions: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at actions.</p> <p>Args</p> <ul> <li>actions (Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/categorical/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the distribution.</p>"},{"location":"api_docs/xplore/distribution/categorical/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = ...\n)\n</code></pre></p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/","title":"DiagonalGaussian","text":""},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#diagonalgaussian","title":"DiagonalGaussian","text":"<p>source <pre><code>DiagonalGaussian(\nloc: th.Tensor, scale: th.Tensor\n)\n</code></pre></p> <p>Diagonal Gaussian distribution for 'Box' tasks.</p> <p>Args</p> <ul> <li>loc (Tensor) : The mean of the distribution (often referred to as mu).</li> <li>scale (Tensor) : The standard deviation of the distribution (often referred to as sigma).</li> </ul> <p>Returns</p> <p>Squashed normal distribution instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nactions: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at actions.</p> <p>Args</p> <ul> <li>actions (Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the distribution.</p>"},{"location":"api_docs/xplore/distribution/diagonal_gaussian/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/kl/","title":"kl","text":""},{"location":"api_docs/xplore/distribution/kl/#kl_categorical_categorical","title":"kl_categorical_categorical","text":"<p>source <pre><code>.kl_categorical_categorical(\np, q\n)\n</code></pre></p>"},{"location":"api_docs/xplore/distribution/kl/#kl_diagonal_gaussian_diagonal_gaussian","title":"kl_diagonal_gaussian_diagonal_gaussian","text":"<p>source <pre><code>.kl_diagonal_gaussian_diagonal_gaussian(\np, q\n)\n</code></pre></p>"},{"location":"api_docs/xplore/distribution/normal_noise/","title":"NormalNoise","text":""},{"location":"api_docs/xplore/distribution/normal_noise/#normalnoise","title":"NormalNoise","text":"<p>source <pre><code>NormalNoise(\nloc: float = 0.0, scale: float = 1.0, stddev_schedule: str = 'linear(1.0, 0.1,\n100000)', stddev_clip: float = 0.3\n)\n</code></pre></p> <p>Gaussian action noise.</p> <p>Args</p> <ul> <li>loc (float) : mean of the noise (often referred to as mu).</li> <li>scale (float) : standard deviation of the noise (often referred to as sigma).</li> <li>stddev_schedule (str) : Use the exploration std schedule.</li> </ul> <p>Returns</p> <p>Gaussian action noise instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nclip: bool = False, sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>clip (bool) : Whether to perform noise truncation.</li> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nvalue: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at <code>value</code>.</p> <p>Args</p> <ul> <li>value (Tensor) : The value to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#reset","title":".reset","text":"<p>source <pre><code>.reset(\nnoiseless_action: th.Tensor, step: int = 0\n)\n</code></pre></p> <p>Reset the noise instance.</p> <p>Args</p> <ul> <li>noiseless_action (Tensor) : Unprocessed actions.</li> <li>step (int) : Global training step that can be None when there is no noise schedule.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/normal_noise/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/","title":"OrnsteinUhlenbeckNoise","text":""},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#ornsteinuhlenbecknoise","title":"OrnsteinUhlenbeckNoise","text":"<p>source <pre><code>OrnsteinUhlenbeckNoise(\nloc: float = 0.0, scale: float = 1.0, theta: float = 0.15, dt: float = 0.01,\nstddev_schedule: str = 'linear(1.0, 0.1, 100000)'\n)\n</code></pre></p> <p>Ornstein Uhlenbeck action noise. Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab</p> <p>Args</p> <ul> <li>loc (float) : mean of the noise (often referred to as mu).</li> <li>scale (float) : standard deviation of the noise (often referred to as sigma).</li> <li>theta (float) : Rate of mean reversion.</li> <li>dt (float) : Timestep for the noise.</li> </ul> <p>Returns</p> <p>Ornstein-Uhlenbeck noise instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#reset","title":".reset","text":"<p>source <pre><code>.reset(\nnoiseless_action: th.Tensor, step: int = 0\n)\n</code></pre></p> <p>Reset the noise instance.</p> <p>Args</p> <ul> <li>noiseless_action (Tensor) : Unprocessed actions.</li> <li>step (int) : Global training step that can be None when there is no noise schedule.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nclip: bool = False, sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample</p> <p>Args</p> <ul> <li>clip (bool) : Range for noise truncation operation.</li> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nvalue: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at <code>value</code>.</p> <p>Args</p> <ul> <li>value (Tensor) : The value to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/ornstein_uhlenbeck_noise/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/","title":"SquashedNormal","text":""},{"location":"api_docs/xplore/distribution/squashed_normal/#squashednormal","title":"SquashedNormal","text":"<p>source <pre><code>SquashedNormal(\nloc: th.Tensor, scale: th.Tensor\n)\n</code></pre></p> <p>Squashed normal distribution for Soft Actor-Critic learner.</p> <p>Args</p> <ul> <li>loc (Tensor) : The mean of the distribution (often referred to as mu).</li> <li>scale (Tensor) : The standard deviation of the distribution (often referred to as sigma).</li> </ul> <p>Returns</p> <p>Squashed normal distribution instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Return the transformed mean.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nactions: th.Tensor\n)\n</code></pre></p> <p>Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.</p> <p>Args</p> <ul> <li>actions (Tensor) : The actions to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/distribution/squashed_normal/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the distribution.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/","title":"TruncatedNormalNoise","text":""},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#truncatednormalnoise","title":"TruncatedNormalNoise","text":"<p>source <pre><code>TruncatedNormalNoise(\nloc: float = 0.0, scale: float = 1.0, stddev_schedule: str = 'linear(1.0, 0.1,\n100000)', stddev_clip: float = 0.3\n)\n</code></pre></p> <p>Truncated normal action noise. See Section 3.1 of \"Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning\".</p> <p>Args</p> <ul> <li>loc (float) : mean of the noise (often referred to as mu).</li> <li>scale (float) : standard deviation of the noise (often referred to as sigma).</li> <li>stddev_schedule (str) : Use the exploration std schedule.</li> <li>stddev_clip (float) : The exploration std clip range.</li> </ul> <p>Returns</p> <p>Truncated normal noise instance.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nclip: bool = False, sample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>clip (bool) : Whether to perform noise truncation.</li> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#rsample","title":".rsample","text":"<p>source <pre><code>.rsample(\nsample_shape: th.Size = th.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> <p>Args</p> <ul> <li>sample_shape (Size) : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#log_prob","title":".log_prob","text":"<p>source <pre><code>.log_prob(\nvalue: th.Tensor\n)\n</code></pre></p> <p>Returns the log of the probability density/mass function evaluated at <code>value</code>.</p> <p>Args</p> <ul> <li>value (Tensor) : The value to be evaluated.</li> </ul> <p>Returns</p> <p>The log_prob value.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#reset","title":".reset","text":"<p>source <pre><code>.reset(\nnoiseless_action: th.Tensor, step: int = 0\n)\n</code></pre></p> <p>Reset the noise instance.</p> <p>Args</p> <ul> <li>noiseless_action (Tensor) : Unprocessed actions.</li> <li>step (int) : Global training step that can be None when there is no noise schedule.</li> </ul> <p>Returns</p> <p>None.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#mean","title":".mean","text":"<p>source <pre><code>.mean()\n</code></pre></p> <p>Returns the mean of the distribution.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#mode","title":".mode","text":"<p>source <pre><code>.mode()\n</code></pre></p> <p>Returns the mode of the distribution.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#entropy","title":".entropy","text":"<p>source <pre><code>.entropy()\n</code></pre></p> <p>Returns the Shannon entropy of distribution.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#stddev","title":".stddev","text":"<p>source <pre><code>.stddev()\n</code></pre></p> <p>Returns the standard deviation of the distribution.</p>"},{"location":"api_docs/xplore/distribution/truncated_normal_noise/#variance","title":".variance","text":"<p>source <pre><code>.variance()\n</code></pre></p> <p>Returns the variance of the distribution.</p>"},{"location":"api_docs/xplore/reward/base/","title":"BaseIntrinsicRewardModule","text":""},{"location":"api_docs/xplore/reward/base/#baseintrinsicrewardmodule","title":"BaseIntrinsicRewardModule","text":"<p>source <pre><code>BaseIntrinsicRewardModule(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05\n)\n</code></pre></p> <p>Base class of intrinsic reward module.</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> </ul> <p>Returns</p> <p>Instance of the base intrinsic reward module.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/base/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/base/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/girm/","title":"GIRM","text":""},{"location":"api_docs/xplore/reward/girm/#girm","title":"GIRM","text":"<p>source <pre><code>GIRM(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, lr: float = 0.001, batch_size: int = 64, lambd: float = 0.5,\nlambd_recon: float = 1.0, lambd_action: float = 1.0, kld_loss_beta: float = 1.0\n)\n</code></pre></p> <p>Intrinsic Reward Driven Imitation Learning via Generative Model (GIRM). See paper: http://proceedings.mlr.press/v119/yu20d/yu20d.pdf</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> <li>lambd (float) : The weighting coefficient for combining actions.</li> <li>lambd_recon (float) : Weighting coefficient of the reconstruction loss.</li> <li>lambd_action (float) : Weighting coefficient of the action loss.</li> <li>kld_loss_beta (float) : Weighting coefficient of the divergence loss.</li> </ul> <p>Returns</p> <p>Instance of GIRM.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/girm/#get_vae_loss","title":".get_vae_loss","text":"<p>source <pre><code>.get_vae_loss(\nrecon_x: th.Tensor, x: th.Tensor, mean: th.Tensor, logvar: th.Tensor\n)\n</code></pre></p> <p>Compute the vae loss.</p> <p>Args</p> <ul> <li>recon_x (Tensor) : Reconstructed x.</li> <li>x (Tensor) : Input x.</li> <li>mean (Tensor) : Sample mean.</li> <li>logvar (Tensor) : Log of the sample variance.</li> </ul> <p>Returns</p> <p>Loss values.</p>"},{"location":"api_docs/xplore/reward/girm/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/girm/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/icm/","title":"ICM","text":""},{"location":"api_docs/xplore/reward/icm/#icm","title":"ICM","text":"<p>source <pre><code>ICM(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, lr: float = 0.001, batch_size: int = 64\n)\n</code></pre></p> <p>Curiosity-Driven Exploration by Self-Supervised Prediction. See paper: http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> </ul> <p>Returns</p> <p>Instance of ICM.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/icm/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/icm/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/ngu/","title":"NGU","text":""},{"location":"api_docs/xplore/reward/ngu/#ngu","title":"NGU","text":"<p>source <pre><code>NGU(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, lr: float = 0.001, batch_size: int = 64, capacity: int = 1000,\nk: int = 10, kernel_cluster_distance: float = 0.008, kernel_epsilon: float = 0.0001,\nc: float = 0.001, sm: float = 8.0, mrs: float = 5.0\n)\n</code></pre></p> <p>Never Give Up: Learning Directed Exploration Strategies (NGU). See paper: https://arxiv.org/pdf/2002.06038</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> <li>capacity (int) : The of capacity the episodic memory.</li> <li>k (int) : Number of neighbors.</li> <li>kernel_cluster_distance (float) : The kernel cluster distance.</li> <li>kernel_epsilon (float) : The kernel constant.</li> <li>c (float) : The pseudo-counts constant.</li> <li>sm (float) : The kernel maximum similarity.</li> <li>mrs (float) : The maximum reward scaling.</li> </ul> <p>Returns</p> <p>Instance of NGU.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/ngu/#pseudo_counts","title":".pseudo_counts","text":"<p>source <pre><code>.pseudo_counts(\ne: th.Tensor\n)\n</code></pre></p> <p>Pseudo counts.</p> <p>Args</p> <ul> <li>e (Tensor) : Encoded observations.</li> </ul> <p>Returns</p> <p>Conut values.</p>"},{"location":"api_docs/xplore/reward/ngu/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/ngu/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/","title":"PseudoCounts","text":""},{"location":"api_docs/xplore/reward/pseudo_counts/#pseudocounts","title":"PseudoCounts","text":"<p>source <pre><code>PseudoCounts(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 32, lr: float = 0.001, batch_size: int = 64, capacity: int = 1000,\nk: int = 10, kernel_cluster_distance: float = 0.008, kernel_epsilon: float = 0.0001,\nc: float = 0.001, sm: float = 8.0\n)\n</code></pre></p> <p>Pseudo-counts based on \"Never Give Up: Learning Directed Exploration Strategies (NGU)\". See paper: https://arxiv.org/pdf/2002.06038</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> <li>capacity (int) : The of capacity the episodic memory.</li> <li>k (int) : Number of neighbors.</li> <li>kernel_cluster_distance (float) : The kernel cluster distance.</li> <li>kernel_epsilon (float) : The kernel constant.</li> <li>c (float) : The pseudo-counts constant.</li> <li>sm (float) : The kernel maximum similarity.</li> </ul> <p>Returns</p> <p>Instance of PseudoCounts.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/#pseudo_counts","title":".pseudo_counts","text":"<p>source <pre><code>.pseudo_counts(\ne: th.Tensor\n)\n</code></pre></p> <p>Pseudo counts.</p> <p>Args</p> <ul> <li>e (Tensor) : Encoded observations.</li> </ul> <p>Returns</p> <p>Conut values.</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/pseudo_counts/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/re3/","title":"RE3","text":""},{"location":"api_docs/xplore/reward/re3/#re3","title":"RE3","text":"<p>source <pre><code>RE3(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, k: int = 5, average_entropy: bool = False\n)\n</code></pre></p> <p>State Entropy Maximization with Random Encoders for Efficient Exploration (RE3). See paper: http://proceedings.mlr.press/v139/seo21a/seo21a.pdf</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>k (int) : Use the k-th neighbors.</li> <li>average_entropy (bool) : Use the average of entropy estimation.</li> </ul> <p>Returns</p> <p>Instance of RE3.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/re3/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/re3/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/revd/","title":"REVD","text":""},{"location":"api_docs/xplore/reward/revd/#revd","title":"REVD","text":"<p>source <pre><code>REVD(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, alpha: float = 0.5, k: int = 5, average_divergence: bool = False\n)\n</code></pre></p> <p>Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning (REVD). See paper: https://openreview.net/pdf?id=V2pw1VYMrDo</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>alpha (alpha) : The order of R\u00e9nyi divergence.</li> <li>k (int) : Use the k-th neighbors.</li> <li>average_divergence (bool) : Use the average of divergence estimation.</li> </ul> <p>Returns</p> <p>Instance of REVD.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/revd/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/revd/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/ride/","title":"RIDE","text":""},{"location":"api_docs/xplore/reward/ride/#ride","title":"RIDE","text":"<p>source <pre><code>RIDE(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, lr: float = 0.001, batch_size: int = 64, capacity: int = 1000,\nk: int = 10, kernel_cluster_distance: float = 0.008, kernel_epsilon: float = 0.0001,\nc: float = 0.001, sm: float = 8.0\n)\n</code></pre></p> <p>RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments. See paper: https://arxiv.org/pdf/2002.12292</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> <li>capacity (int) : The of capacity the episodic memory.</li> <li>k (int) : Number of neighbors.</li> <li>kernel_cluster_distance (float) : The kernel cluster distance.</li> <li>kernel_epsilon (float) : The kernel constant.</li> <li>c (float) : The pseudo-counts constant.</li> <li>sm (float) : The kernel maximum similarity.</li> </ul> <p>Returns</p> <p>Instance of RIDE.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/ride/#pseudo_counts","title":".pseudo_counts","text":"<p>source <pre><code>.pseudo_counts(\ne: th.Tensor\n)\n</code></pre></p> <p>Pseudo counts.</p> <p>Args</p> <ul> <li>e (Tensor) : Encoded observations.</li> </ul> <p>Returns</p> <p>Conut values.</p>"},{"location":"api_docs/xplore/reward/ride/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/ride/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/rise/","title":"RISE","text":""},{"location":"api_docs/xplore/reward/rise/#rise","title":"RISE","text":"<p>source <pre><code>RISE(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, alpha: float = 0.5, k: int = 5, average_entropy: bool = False\n)\n</code></pre></p> <p>R\u00e9nyi State Entropy Maximization for Exploration Acceleration in Reinforcement Learning (RISE). See paper: https://ieeexplore.ieee.org/abstract/document/9802917/</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>alpha (alpha) : The The order of R\u00e9nyi entropy.</li> <li>k (int) : Use the k-th neighbors.</li> <li>average_entropy (bool) : Use the average of entropy estimation.</li> </ul> <p>Returns</p> <p>Instance of RISE.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/rise/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/rise/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"api_docs/xplore/reward/rnd/","title":"RND","text":""},{"location":"api_docs/xplore/reward/rnd/#rnd","title":"RND","text":"<p>source <pre><code>RND(\nobservation_space: Union[gym.Space, DictConfig], action_space: Union[gym.Space,\nDictConfig], device: str = 'cpu', beta: float = 0.05, kappa: float = 2.5e-05,\nlatent_dim: int = 128, lr: float = 0.001, batch_size: int = 64\n)\n</code></pre></p> <p>Exploration by Random Network Distillation (RND). See paper: https://arxiv.org/pdf/1810.12894.pdf</p> <p>Args</p> <ul> <li>observation_space (Space or DictConfig) : The observation space of environment. When invoked by Hydra,     'observation_space' is a 'DictConfig' like {\"shape\": observation_space.shape, }.</li> <li>action_space (Space or DictConfig) : The action space of environment. When invoked by Hydra,     'action_space' is a 'DictConfig' like     {\"shape\": (n, ), \"type\": \"Discrete\", \"range\": [0, n - 1]} or     {\"shape\": action_space.shape, \"type\": \"Box\", \"range\": [action_space.low[0], action_space.high[0]]}.</li> <li>device (str) : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta (float) : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa (float) : The decay rate.</li> <li>latent_dim (int) : The dimension of encoding vectors.</li> <li>lr (float) : The learning rate.</li> <li>batch_size (int) : The batch size for update.</li> </ul> <p>Returns</p> <p>Instance of RND.</p> <p>Methods:</p>"},{"location":"api_docs/xplore/reward/rnd/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nsamples: Dict, step: int = 0\n)\n</code></pre></p> <p>Compute the intrinsic rewards for current samples.</p> <p>Args</p> <ul> <li>samples (Dict) : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <li>step (int) : The global training step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api_docs/xplore/reward/rnd/#update","title":".update","text":"<p>source <pre><code>.update(\nsamples: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>samples  : The collected samples. A python dict like     {obs (n_steps, n_envs, obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs) ,     next_obs (n_steps, n_envs, *obs_shape) }. <p>Returns</p> <p>None</p>"},{"location":"tutorials/","title":"Overview","text":"<p>Welcome to Hsuanwu tutorials!\ud83d\udc4b\ud83d\udc4b\ud83d\udc4b</p> <ul> <li>Quick Start: Build your first Hsuanwu application</li> <li>Write a configuration file </li> <li>Use intrinsic reward and observation augmentation</li> <li>Make a custom RL environment</li> <li>Use a custom module in Hsuanwu</li> <li>Pre-training in Hsuanwu</li> <li>Evaluate your model</li> <li>Deploy your model in inference devices</li> </ul>"},{"location":"tutorials/configuration/","title":"Configuration","text":""},{"location":"tutorials/configuration/#minimum-config","title":"Minimum Config","text":"<p>As shown in Quick Start, Hsuanwu uses Hydra to manage RL applications elegantly. To build a RL application, the  minimum configuration file can be minimum_config.yaml<pre><code>device: cuda:0         # Device (cpu, cuda, ...).\nseed: 1                # Random seed for reproduction.\nnum_train_steps: 5000  # Number of training steps.\nagent:\nname: DrQv2          # The agent name.\n</code></pre> This file will indicate the running device, random seed, training steps, and RL agent.</p>"},{"location":"tutorials/configuration/#complete-parameter-support-list","title":"Complete Parameter Support List","text":"<p>If you want to further specify other parameters, this is a complete list of parameters: complete_config.yaml<pre><code>####### Train setup\ndevice: cuda:0                   # Device (cpu, cuda, ...) on which the code should be run.\nseed: 1                          # Random seed for reproduction.\npretraining: false               # Turn on the pre-training mode.\ninit_model_path: ...             # Path of initial model parameters.\nnum_train_steps: 250000          # Number of training steps.\nnum_init_steps: 2000             # Number of initial exploration steps, only for `off-policy` agents.\n####### Test setup\ntest_every_steps: 5000           # Testing interval, only for `off-policy` agents.\ntest_every_episodes: 10          # Testing interval, only for `on-policy` agents.\nnum_test_episodes: 10            # Number of testing episodes.\n####### Choose the encoder\nencoder:\nname: TassaCnnEncoder           # The encoder name. Supported types: https://docs.hsuanwu.dev/api/\nfeature_dim: 50                 # The dimension of extracted features.\n...\n####### Choose the agent\nlearner:\nname: DrQv2                     # The agent name. Supported types: https://docs.hsuanwu.dev/api/\n...\n####### Choose the storage\nstorage:\nname: NStepReplayStorage        # The storage name. Supported types: https://docs.hsuanwu.dev/api/\n...\n####### Choose the distribution\ndistribution:\nname: TruncatedNormalNoise      # The distribution name. Supported types: https://docs.hsuanwu.dev/api/\n...\n####### Choose the augmentation\naugmentation:\nname: RandomShift               # The augmentation name. Supported types: https://docs.hsuanwu.dev/api/\n...\n####### Choose the intrinsic reward\nreward:\nname: RE3                       # The reward name. Supported types: https://docs.hsuanwu.dev/api/\n...\nhydra:\nrun:\ndir: ./logs/${experiment}/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname} # Specify the output directory.\njob:\nchdir: true # Change the working working.\n</code></pre> For a specified module, you can further specify its internal parameters based on API Documentation.</p>"},{"location":"tutorials/configuration/#override-values","title":"Override Values","text":"<p>example.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\nagent:\nname: DrQv2             # The agent name.\n</code></pre> For a written config, if you want to override some values, it suffices to: <pre><code>python train.py seed=7\n</code></pre> Meanwhile, use <code>+</code> to add a new value if it isn't in the config: <pre><code>python train.py seed=7 +num_init_steps=3000\n</code></pre></p>"},{"location":"tutorials/configuration/#multirun","title":"Multirun","text":"<p>Hydra allows us to conveniently run the same application with multiple different configurations: <pre><code>python train.py --multirun seed=7,8,9\n\n[2023-04-30 13:28:00,466][HYDRA] Launching 3 jobs locally\n[2023-04-30 13:28:00,351][HYDRA]    #0 : seed=7\n[2023-04-30 13:28:01,679][HYDRA]    #0 : seed=8\n[2023-04-30 13:28:02,218][HYDRA]    #0 : seed=9\n</code></pre></p>"},{"location":"tutorials/custom_environment/","title":"Custom environment","text":""},{"location":"tutorials/custom_environment/#defining-the-environment","title":"Defining the Environment","text":"<p>To use custom environments in Hsuanwu, it suffices to follow the gymnasium interface and prepare your environment following Make your own custom environment. Next, write a  <code>make_env</code> function like make_env<pre><code>def make_env():\ndef _thunk():\nreturn gym.make(\"Acrobot-v1\")\nreturn _thunk\n</code></pre></p>"},{"location":"tutorials/custom_environment/#use-hsuanwuenvwrapper","title":"Use <code>HsuanwuEnvWrapper</code>","text":"<p>In Hsuanwu, the environments are assumed to be vectorized and a <code>HsuanwuEnvWrapper</code> is example.py<pre><code>from hsuanwu.env.utils import HsuanwuEnvWrapper\n# create vectorized environments\nnum_envs = 7\ngym_env = gym.vector.SyncVectorEnv([make_env() for _ in range(num_envs)])\ngym_env = gym.wrappers.RecordEpisodeStatistics(gym_env)\n# wrap the environments\ntrain_env = HsuanwuEnvWrapper(gym_env, device='cpu')\n</code></pre> After that, you can use the custom environment in application directly.</p>"},{"location":"tutorials/data_augmentation/","title":"Data augmentation","text":""},{"location":"tutorials/data_augmentation/#write-configuration-file","title":"Write Configuration File","text":"<p>Hsuanwu supports the intrinsic reward-driven exploration and observation augmentation by default. To invoke them, it suffices to  add the following contents in the config: config.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\nagent:\nname: DrQv2             # The agent name.\n</code></pre> The adjusted config: new_config.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\nagent:\nname: DrQv2             # The agent name.\n####### Choose the augmentation\naugmentation:\nname: RandomShift        # The augmentation name. Supported types: https://docs.hsuanwu.dev/api/\n...\n####### Choose the intrinsic reward\nreward:\nname: RE3                # The reward name. Supported types: https://docs.hsuanwu.dev/api/\n...\n</code></pre> For supported modules, see API Documentation.</p>"},{"location":"tutorials/data_augmentation/#intro-to-intrinsic-reward-modules","title":"Intro to Intrinsic Reward Modules","text":"<p>Due to the large differences in the calculation of different intrinsic reward methods, Hsuanwu has the following rules:</p> <ol> <li>The environments are assumed to be vectorized;</li> <li>The compute_irs function of each intrinsic reward module has a mandatory argument samples, which is a dict like:<ul> <li>obs (n_steps, n_envs, *obs_shape)  <li>actions (n_steps, n_envs, action_shape)  <li>rewards (n_steps, n_envs)  <li>next_obs (n_steps, n_envs, *obs_shape)  <p>Take RE3 for instance, it computes the intrinsic reward for each state based on the Euclidean distance between the state and  its \\(k\\)-nearest neighbor within a mini-batch. Thus it suffices to provide obs data to compute the reward. The following code provides a usage example of RE3: example.py<pre><code>from hsuanwu.xplore.reward import RE3\nfrom hsuanwu.env import make_dmc_env\nimport torch as th\nif __name__ == '__main__':\nnum_envs = 7\nnum_steps = 128\n# create env\nenv = make_dmc_env(env_id=\"cartpole_balance\", num_envs=num_envs)\nprint(env.observation_space, env.action_space)\n# create RE3 instance\nre3 = RE3(\nobservation_space=env.observation_space,\naction_space=env.action_space\n)\n# compute intrinsic rewards\nobs = th.rand(size=(num_steps, num_envs, *env.observation_space.shape))\nintrinsic_rewards = re3.compute_irs(samples={'obs': obs})\nprint(intrinsic_rewards.shape, type(intrinsic_rewards))\nprint(intrinsic_rewards)\n# Output:\n# {'shape': [9, 84, 84]} {'shape': [1], 'type': 'Box', 'range': [-1.0, 1.0]}\n# torch.Size([128, 7]) &lt;class 'torch.Tensor'&gt;\n</code></pre></p>"},{"location":"tutorials/evaluation/","title":"Evaluation","text":"<p>Hsuanwu provides evaluation methods based on:</p> <p>Agarwal R, Schwarzer M, Castro P S, et al. Deep reinforcement learning at the edge of the statistical precipice[J]. Advances in neural information processing systems, 2021, 34: 29304-29320.</p> <p>We reconstruct and improve the code of the official repository rliable, achieving higher convenience and efficiency.</p>"},{"location":"tutorials/evaluation/#download-data","title":"Download Data","text":"<ul> <li>Suppose we want to evaluate algorithm performance on the Procgen benchmark. First, download the data from  HsuanwuHub: <pre><code>pip install hsuanwuhub\n</code></pre></li> <li>Load data: example.py<pre><code>from hsuanwuhub import datasets\nfrom hsuanwu.evaluation import Performance, Comparison, min_max_normalize\nimport numpy as np\nprocgen = datasets.Procgen()\nprocgen_scores = procgen.load_scores()\nppo_scores, ppg_scores = procgen_scores['PPO'], procgen_scores['PPG']\n# PPO-Normalized scores\nnorm_ppo_scores = min_max_normalize(ppo_scores, \nmin_scores=np.zeros_like(ppo_scores), \nmax_scores=np.mean(ppo_scores, axis=0))\nnorm_ppg_scores = min_max_normalize(ppg_scores, \nmin_scores=np.zeros_like(ppo_scores), \nmax_scores=np.mean(ppo_scores, axis=0))\n</code></pre> For each algorithm, this will return a <code>NdArray</code> of size (<code>10</code> x <code>16</code>) where scores[n][m] represent the score on run <code>n</code> of task <code>m</code>.</li> </ul>"},{"location":"tutorials/evaluation/#performance-evaluation","title":"Performance Evaluation","text":"<p>Import performance evaluator: example.py<pre><code>perf = Performance(scores=norm_ppo_scores, \nget_ci=True # get confidence intervals\n)\nperf.aggregate_mean()\n# Output:\n# Computing confidence interval for aggregate MEAN...\n# (1.0, array([[0.9737281 ], [1.02564405]]))\n</code></pre> Available metrics:</p> Metric Remark <code>.aggregate_mean</code> Computes mean of sample mean scores per task. <code>.aggregate_median</code> Computes median of sample mean scores per task. <code>.aggregate_og</code> Computes optimality gap across all runs and tasks. <code>.aggregate_iqm</code> Computes the interquartile mean across runs and tasks."},{"location":"tutorials/evaluation/#performance-comparison","title":"Performance Comparison","text":"<p><code>Comparison</code> module allows you to compare the performance between two algorithms: example.py<pre><code>comp = Comparison(scores_x=norm_ppg_scores,\nscores_y=norm_ppo_scores,\nget_ci=True)\ncomp.compute_poi()\n# Output:\n# Computing confidence interval for PoI...\n# (0.8153125, array([[0.779375  ], [0.85000781]]))\n</code></pre> This indicates the overall probability of imporvement of <code>PPG</code> over <code>PPO</code> is <code>0.8153125</code>.</p> <p>Available metrics:</p> Metric Remark <code>.compute_poi</code> Compute the overall probability of imporvement of algorithm <code>X</code> over <code>Y</code>."},{"location":"tutorials/evaluation/#visualization","title":"Visualization","text":""},{"location":"tutorials/pre-training/","title":"Pre-training","text":""},{"location":"tutorials/pre-training/#training-configuration","title":"Training Configuration","text":"<p>Currently, Hsuanwu only supports online pre-training via intrinsic reward. To turn on the pre-training mode,  it suffices to adjust the config file like: config.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\nagent:\nname: DrQv2             # The agent name.\n</code></pre> The adjusted config: new_config.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\npretraining: true         # turn on the pre-training mode.\nagent:\nname: DrQv2             # The agent name.\n####### Choose the intrinsic reward\nreward:\nname: RE3                # The reward name.\n...\n</code></pre> Run the <code>train.py</code>, and you will see the following output: <pre><code>[04/30/2023 02:58:56 PM] - [HSUANWU INFO ] - Experiment: drqv2_dmc_pixel\n[04/30/2023 02:58:56 PM] - [HSUANWU INFO ] - Invoking Hsuanwu Engine...\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Checking the Compatibility of Modules...\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Selected Encoder: TassaCnnEncoder\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Selected Agent: DrQv2\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Selected Storage: NStepReplayStorage\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Selected Distribution: TruncatedNormalNoise\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Use Augmentation: True, RandomShift\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Use Intrinsic Reward: True, RE3\n[04/30/2023 02:58:56 PM] - [HSUANWU INFO ] - Deploying OffPolicyTrainer...\n[04/30/2023 02:58:56 PM] - [HSUANWU INFO ] - Pre-training Mode On...\n[04/30/2023 02:58:56 PM] - [HSUANWU DEBUG] - Check Accomplished. Start Training...\n</code></pre></p> <p>Tip</p> <p>When the pre-training mode is on, a <code>reward</code> module must be specified!</p> <p>For all supported reward modules, see API Documentation.</p>"},{"location":"tutorials/pre-training/#fine-tuning","title":"Fine-tuning","text":"<p>Once the pre-training is finished, you can find the model parameters in the <code>pretrained</code> subfolder of the working directory. To  load the parameters, just turn off the pre-training mode and write the config file like new_config.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\npretraining: false        # turn off the pre-training mode.\ninit_model_path: ...      # Path of initial model parameters.\nagent:\nname: DrQv2             # The agent name.\n</code></pre> Run the <code>train.py</code>, and you will see the following output: <pre><code>[04/30/2023 03:54:25 PM] - [HSUANWU INFO ] - Experiment: drqv2_dmc_pixel\n[04/30/2023 03:54:25 PM] - [HSUANWU INFO ] - Invoking Hsuanwu Engine...\n[04/30/2023 03:54:25 PM] - [HSUANWU DEBUG] - Checking the Compatibility of Modules...\n[04/30/2023 03:54:25 PM] - [HSUANWU DEBUG] - Selected Encoder: TassaCnnEncoder\n[04/30/2023 03:54:25 PM] - [HSUANWU DEBUG] - Selected Agent: DrQv2\n[04/30/2023 03:54:25 PM] - [HSUANWU DEBUG] - Selected Storage: NStepReplayStorage\n[04/30/2023 03:54:25 PM] - [HSUANWU DEBUG] - Selected Distribution: TruncatedNormalNoise\n[04/30/2023 03:54:25 PM] - [HSUANWU DEBUG] - Use Augmentation: True, RandomShift\n[04/30/2023 03:54:25 PM] - [HSUANWU DEBUG] - Use Intrinsic Reward: False\n[04/30/2023 03:54:25 PM] - [HSUANWU INFO ] - Deploying OffPolicyTrainer...\n[04/30/2023 03:54:26 PM] - [HSUANWU INFO ] - Loading Initial Parameters from /home/yuanmingqi/code/Hsuanwu/logs/drqv2_dmc_pixel/2023.04.30/155249_/pretrained\n[04/30/2023 03:54:26 PM] - [HSUANWU DEBUG] - Check Accomplished. Start Training...\n</code></pre></p>"},{"location":"tutorials/quick_start/","title":"Quick start","text":""},{"location":"tutorials/quick_start/#build-application","title":"Build Application","text":"<p>Hsuanwu uses Hydra to manage RL applications elegantly. For example,  we want to use DrQ-v2 to solve a task of DeepMind Control Suite, and we only need the following two steps:</p> <ol> <li> <p>Write a <code>config.yaml</code> file in your working directory like: config.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\nagent:\nname: DrQv2             # The agent name.\n</code></pre></p> </li> <li> <p>Write a train.py file like: train.py<pre><code>import hydra # Use Hydra to manage experiments\nfrom hsuanwu.common.engine import HsuanwuEngine # Import Hsuanwu engine\ntrain_env = make_dmc_env(env_id='cartpole_balance') # Create train env\ntest_env = make_dmc_env(env_id='cartpole_balance') # Create test env [Optional]\n@hydra.main(version_base=None, config_path='./', config_name='config')\ndef main(cfgs):\nengine = HsuanwuEngine(cfgs=cfgs, train_env=train_env, test_env=test_env) # Initialize engine\nengine.invoke() # Start training\nif __name__ == '__main__':\nmain()\n</code></pre></p> </li> </ol>"},{"location":"tutorials/quick_start/#start-training","title":"Start Training","text":"<p>Run <code>train.py</code> and you will see the following output:</p> <p>Read the logs</p> <ul> <li>S: Number of environment steps. Note that <code>S</code> isn't equal to the number of frames in visual tasks, and <code>number_of_frames=number_of_steps * number_of_action_repeats</code></li> <li>E: Number of environment episodes.</li> <li>L: Average episode length.</li> <li>R: Average episode reward.</li> <li>FPS: Training FPS.</li> <li>T: Time costs.</li> </ul>"},{"location":"tutorials/quick_start/#outputworking-directory","title":"Output/Working Directory","text":"<p>You can specify the working directory by  <code>config.yaml</code>, or an <code>outputs</code> folder will be used by default. config.yaml<pre><code>experiment: drqv2_dmc     # Experiment ID.\ndevice: cuda:0            # Device (cpu, cuda, ...).\nseed: 1                   # Random seed for reproduction.\nnum_train_steps: 5000     # Number of training steps.\nagent:\nname: DrQv2             # The agent name.\nhydra:\nrun:\ndir: ./logs/${experiment}/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname} # Specify the output directory.\njob:\nchdir: true # Change the working working.\n</code></pre></p>"},{"location":"tutorials/quick_start/#load-the-trained-model","title":"Load the Trained Model","text":"<p>Once the training is finished, you can find <code>encoder.pth</code> and <code>actor.pth</code> in the subfolder <code>model</code> of the specified working directory.</p> play.py<pre><code>import torch as th\n# load the model and specify the map location\nencoder = th.load(\"encoder.pth\", map_location=th.device('cpu'))\nactor = th.load(\"actor.pth\", map_location=th.device('cpu'))\nobs = th.zeros(size=(1, 9, 84, 84))\naction = actor(encoder(obs))\nprint(action)\n# Output: tensor([[-1.0000]], grad_fn=&lt;TanhBackward0&gt;)\n</code></pre>"}]}